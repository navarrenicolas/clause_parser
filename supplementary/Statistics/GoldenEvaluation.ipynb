{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a1a894-5b0f-47bd-af5b-d599703f5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.ClauseParser import ClauseParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4261546-559f-4063-a1e2-cc564ad96a18",
   "metadata": {},
   "source": [
    "## Parsing tools and data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013b9739-86d3-413a-b246-5ab32e4aed54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "parser = ClauseParser()\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "# Function for quick sentence processing\n",
    "def nlp_sents(string):\n",
    "    return list(nlp(string).sents)\n",
    "\n",
    "# Golden Data file paths\n",
    "adv_path_golden = \"../Annotation/adversarials_golden_set.json\"\n",
    "flat_path_golden = \"../Annotation/golden_sets_flattened.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bc2859-7f68-45b4-8d92-c09b70c45c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_test = pd.read_json(adv_path_golden, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1b370-237f-47a8-9992-5d766a68bba9",
   "metadata": {},
   "source": [
    "## Parse Golden sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9883d040-97b4-4691-a344-07bccaa2a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def parse_flat_golden(filename:str):\n",
    "    golden_df = pd.read_json(filename, orient = 'index')\n",
    "    golden_parses = []\n",
    "    parser_parses = []\n",
    "    for sent in golden_df.sentence.value_counts().to_dict().keys() :\n",
    "        parses = [dict(row) for i,row in (golden_df[golden_df.sentence == sent]).iterrows()]\n",
    "        golden_parses.append(parses)\n",
    "        sent_doc = nlp(sent)\n",
    "        parsed_sent = list(sent_doc.sents)[0]\n",
    "        parser_parses.append(parser.parse_clauses(parsed_sent))\n",
    "    return (parser_parses, golden_parses)\n",
    "\n",
    "flat_parsed, flat_golden = parse_flat_golden(flat_path_golden)\n",
    "adv_parsed, adv_golden = parse_flat_golden(adv_path_golden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbd5ee-2cb4-40bf-a306-8344f99990d3",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d67ff3-f32e-4e39-a295-d4548209b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_parses = sum([len(p) for p in adv_parsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b447a9-637d-4051-8eb9-1efe53d327b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicate_string(predicate):\n",
    "    if len(predicate) ==0:\n",
    "        return ''\n",
    "    pred_string = ''\n",
    "    for item in predicate:\n",
    "        pred_string += str(item['lemma']) + ' '\n",
    "    return pred_string[:-1]\n",
    "\n",
    "\n",
    "def filter_sentences_idx(filt):\n",
    "    return [idx for idx in [i for i, e in enumerate(flat_golden) if filt(e)] ]\n",
    "\n",
    "single_idx = filter_sentences_idx(lambda x: len(x) == 1)\n",
    "multiple_idx = filter_sentences_idx(lambda x: len(x) > 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8d489-e8ff-42bf-b982-a2a2b1a6e846",
   "metadata": {},
   "source": [
    "### F1-scores for clause detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2fae38-9fc0-42bc-9a43-b0b95ac11149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(tp, fp, fn):\n",
    "  precision = tp/(tp+fp)\n",
    "  recall = tp/(tp+fn)\n",
    "  f1 = (2*precision*recall)/(precision + recall)\n",
    "  return [precision, recall, f1]\n",
    "\n",
    "def get_stats(parser,golden,subset='overall'):\n",
    "    \n",
    "    tp = 0\n",
    "    tn = len(adv_golden) - adv_parses if subset == 'overall' else 0\n",
    "    fp = adv_parses if subset == 'overall' else 0\n",
    "    fn = 0 \n",
    "    \n",
    "    for i in range(len(golden)):\n",
    "\n",
    "        \n",
    "            \n",
    "        gp = golden[i]\n",
    "        bp = parser[i]\n",
    "        \n",
    "        if subset == 'single':\n",
    "            if len(gp) != 1:\n",
    "                continue\n",
    "        elif subset == 'multiple':\n",
    "            if len(gp) == 1:\n",
    "                continue\n",
    "        \n",
    "        if len(gp) == len(bp):\n",
    "            tp += len(gp)\n",
    "        elif len(gp) > len(bp):\n",
    "            fn += len(gp) - len(bp)\n",
    "            tp += len(bp)\n",
    "        elif len(gp) < len(bp):\n",
    "            fp += len(bp) - len(gp)\n",
    "            tp += len(gp)\n",
    "    \n",
    "    print(f\"---------Embedded Clause Detection, {subset} Clause F1---------\")\n",
    "    print(\"True Positives: \", tp, \"\\nFalse Positives: \", fp, \"\\nFalse Negatives: \", fn)\n",
    "    print(\"Precision, Recall, F1: \", get_f1(tp, fp, fn))\n",
    "    print(\"Accuracy: \", (tp+tn)/(tp+fp+fn+tn))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37088edb-a8dc-427c-8653-e06a0cace5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Embedded Clause Detection, single Clause F1---------\n",
      "True Positives:  357 \n",
      "False Positives:  40 \n",
      "False Negatives:  21\n",
      "Precision, Recall, F1:  [0.8992443324937027, 0.9444444444444444, 0.9212903225806451]\n",
      "Accuracy:  0.854066985645933\n",
      "\n",
      "---------Embedded Clause Detection, multiple Clause F1---------\n",
      "True Positives:  123 \n",
      "False Positives:  7 \n",
      "False Negatives:  26\n",
      "Precision, Recall, F1:  [0.9461538461538461, 0.825503355704698, 0.881720430107527]\n",
      "Accuracy:  0.7884615384615384\n",
      "\n",
      "---------Embedded Clause Detection, overall Clause F1---------\n",
      "True Positives:  480 \n",
      "False Positives:  53 \n",
      "False Negatives:  47\n",
      "Precision, Recall, F1:  [0.900562851782364, 0.9108159392789373, 0.9056603773584905]\n",
      "Accuracy:  0.8540145985401459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_stats(flat_parsed,flat_golden,'single')\n",
    "get_stats(flat_parsed,flat_golden,'multiple')\n",
    "get_stats(flat_parsed,flat_golden,'overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c22b13-b281-4c3e-bd30-01daa1a4958d",
   "metadata": {},
   "source": [
    "## Overall Feature Identification Acuraccy\n",
    "\n",
    "Identification means that the parsed is able to extract the same features as the golden set\n",
    "\n",
    "- Clause\n",
    "- Clause type\n",
    "- Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2859e0-0cc2-4101-b7a4-26d088833832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_true_detections(parser,golden,subset='overall'):\n",
    "    tp = 0  \n",
    "    for i in range(len(golden)):\n",
    "        gp = golden[i]\n",
    "        bp = parser[i]\n",
    "        if len(gp) == len(bp):\n",
    "            tp += len(gp)\n",
    "        elif len(gp) > len(bp):\n",
    "            tp += len(bp)\n",
    "        elif len(gp) < len(bp):\n",
    "            tp += len(gp)\n",
    "    return tp\n",
    "\n",
    "def compare_data(parsed,golden,feature):\n",
    "    matches = 0\n",
    "    \n",
    "    for parsed,gold in zip(parsed,golden):\n",
    "        \n",
    "        if feature=='predicate':\n",
    "            gold_pred_lemmas = [get_predicate_string(gp['predicate']) for gp in gold]\n",
    "            parsed_pred_lemmas = [get_predicate_string(parse['predicate']) for parse in parsed]\n",
    "            \n",
    "            for pred_lemma in gold_pred_lemmas:\n",
    "                \n",
    "                if pred_lemma in parsed_pred_lemmas:\n",
    "                    matches += 1 \n",
    "                    parsed_pred_lemmas.remove(pred_lemma)\n",
    "        \n",
    "        else:    \n",
    "            gold_features = [gp[feature] for gp in gold]\n",
    "            parsed_features = [parse[feature] for parse in parsed]\n",
    "            \n",
    "            for gold_feature in gold_features:\n",
    "                \n",
    "                if gold_feature in parsed_features:\n",
    "                    matches += 1 \n",
    "                    parsed_features.remove(gold_feature)\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647558cd-617b-40ab-8f32-39c68fba51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy(parsed,golden):\n",
    "    total_detects = count_true_detections(parsed,golden)\n",
    "\n",
    "    # clause identification\n",
    "    correct_clauses = compare_data(parsed,golden,'clause')\n",
    "    print('clause accuracy', correct_clauses/total_detects)\n",
    "    \n",
    "    # Predicate detection\n",
    "    correct_predicates = compare_data(parsed,golden,'predicate')\n",
    "    print('predicate accuracy', correct_predicates/total_detects)\n",
    "    \n",
    "    # Type\n",
    "    correct_types = compare_data(parsed,golden,'type')\n",
    "    print('type accuracy', correct_types/total_detects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a34d7-850b-4d9d-aaa4-7d09c745d0c2",
   "metadata": {},
   "source": [
    "### Overall Identification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56446c8b-30df-4dfb-9da8-776cc456dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clause accuracy 0.86875\n",
      "predicate accuracy 0.9104166666666667\n",
      "type accuracy 0.9604166666666667\n"
     ]
    }
   ],
   "source": [
    "show_accuracy(flat_parsed,flat_golden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d4af3-6df9-44f5-b473-d4b237c5e11c",
   "metadata": {},
   "source": [
    "### Single Clause Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b333619-134d-4210-84d3-becc61f195f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clause accuracy 0.8795518207282913\n",
      "predicate accuracy 0.9663865546218487\n",
      "type accuracy 0.969187675070028\n"
     ]
    }
   ],
   "source": [
    "parsed_single = [flat_parsed[i] for i in single_idx]\n",
    "golden_single = [flat_golden[i] for i in single_idx]\n",
    "\n",
    "show_accuracy(parsed_single,golden_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea9fcf-ee49-42d0-a0d1-71eb3967d624",
   "metadata": {},
   "source": [
    "### Multiple Clause Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70d873fc-71c6-4407-8d23-dedaafc63229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clause accuracy 0.8373983739837398\n",
      "predicate accuracy 0.7479674796747967\n",
      "type accuracy 0.9349593495934959\n"
     ]
    }
   ],
   "source": [
    "parsed_multiple = [flat_parsed[i] for i in multiple_idx]\n",
    "golden_multiple = [flat_golden[i] for i in multiple_idx]\n",
    "\n",
    "show_accuracy(parsed_multiple,golden_multiple)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benepar_env",
   "language": "python",
   "name": "benepar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
