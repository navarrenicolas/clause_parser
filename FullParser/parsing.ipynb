{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23b74f2-09e6-464b-b798-40f4f8da55e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /Users/s2518809/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import benepar, spacy\n",
    "spacy.load('en_core_web_md')\n",
    "benepar.download('benepar_en3')\n",
    "\n",
    "#nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "import re\n",
    "from typing import List, Set, Dict, Tuple\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e5a56-fed7-4638-9c8e-d947ccd02898",
   "metadata": {},
   "source": [
    "\n",
    "## Import the examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef5d88-a058-45a2-84d0-72deb3a7364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decl_samples = open('../Datasets/ClauseEmbeddingExamples/finite-declarative-clauses.txt').read().replace('\\n', ' ')\n",
    "pol_samples = open('../Datasets/ClauseEmbeddingExamples/finite-polar-interrogative-clauses.txt').read().replace('\\n', ' ')\n",
    "const_samples = open('../Datasets/ClauseEmbeddingExamples/finite-consituent-interrogative-clauses.txt').read().replace('\\n', ' ')\n",
    "alt_samples = open('../Datasets/ClauseEmbeddingExamples/finite-alternative-interrogative-clauses.txt').read().replace('\\n', ' ')\n",
    "adv_samples = open('../Datasets/ClauseEmbeddingExamples/adversarial.txt').read().replace('\\n', ' ')\n",
    "all_examples = decl_samples + pol_samples + const_samples + alt_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142474b-06f9-41b5-ae7c-71637759cc0a",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153b89e-37c4-4499-a6f9-9a15430ea150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClauseParser():\n",
    "  \"\"\"\n",
    "  Currently, the code takes in an individual string of all the sentences\n",
    "  and then splits into the invidiual sentence. Returns a List of dictionaries\n",
    "  with the following dictionary structure {sentence: sentence, predicate: predicate,\n",
    "  clause_type: clause_type, clause: clause}. If the class does not find SBAR or a\n",
    "  predicate, the example is completely ignored and not included in the final output\n",
    "  result. If the code does not find a clause or clause type, None is return for\n",
    "  those two variables, however they are still included in the final output results,\n",
    "  just as clause_type: None or clause: None\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    return\n",
    "\n",
    "  def find_clause_type(self, mark_text: str, sent: str) -> str:\n",
    "    if mark_text == \"that\":\n",
    "      return \"declarative\"\n",
    "    elif mark_text == \"which\":\n",
    "      return \"constituent\"\n",
    "    elif mark_text == \"whether\" or mark_text == \"if\":\n",
    "      for token in sent:\n",
    "        if str(token) == \"or\":\n",
    "          if str(token.nbor()) == \"not\":\n",
    "            return \"polar\"\n",
    "          else:\n",
    "            return \"alternative\"\n",
    "      else:\n",
    "        return \"polar\"\n",
    "    return None\n",
    "\n",
    "  def find_clause_with_mark(self, sent_dp: dict, sent: str) -> str:\n",
    "    for arc in sent_dp['arcs']:\n",
    "      if re.search(r\"mark\", arc[\"label\"]):\n",
    "        clause_type = self.find_clause_type(sent_dp[\"words\"][arc[\"start\"]]['text'], sent)\n",
    "        clause_start = arc[\"start\"]\n",
    "        clause = \"\"\n",
    "        for i in range(len(sent_dp[\"words\"])):\n",
    "          if i >= clause_start:\n",
    "            clause += sent_dp[\"words\"][i][\"text\"]\n",
    "            clause += \" \"\n",
    "        return clause.strip(), clause_type\n",
    "    return None, None\n",
    "\n",
    "  def find_clause_without_mark(self, sent_dp: dict) -> str:\n",
    "    for arc in sent_dp['arcs']:\n",
    "      if re.search(r\"(c|p|x)comp\", arc[\"label\"]):\n",
    "        clause_start = arc[\"start\"] + 1\n",
    "        clause = \"\"\n",
    "        for i in range(len(sent_dp[\"words\"])):\n",
    "          if i >= clause_start:\n",
    "            clause += sent_dp[\"words\"][i][\"text\"]\n",
    "            clause += \" \"\n",
    "        return clause.strip()\n",
    "    return None\n",
    "\n",
    "  def confirm_mark(self, sent_dp: dict) -> bool:\n",
    "    for arc in sent_dp['arcs']:\n",
    "      if arc[\"label\"] == \"mark\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "  def confirm_about(self, predicate: str) -> bool:\n",
    "    if predicate == \"about\":\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "  def find_pos(self, predicate: str) -> str:\n",
    "    predicate_nlp = nlp(predicate)\n",
    "    for token in predicate_nlp:\n",
    "      return token.pos_\n",
    "\n",
    "  def find_predicate(self, sent_dp: dict) -> str:\n",
    "    for arc in sent_dp['arcs']:\n",
    "      if re.search(r\"(c|p|x)comp\", arc[\"label\"]):\n",
    "        predicate_index = arc[\"start\"]\n",
    "        if self.confirm_about(sent_dp[\"words\"][predicate_index]['text']) == True:\n",
    "          for inner_arc in sent_dp['arcs']:\n",
    "            if inner_arc['end'] == predicate_index:\n",
    "              return {\"predicate\": sent_dp[\"words\"][inner_arc[\"start\"]]['text'], \"POS\": self.find_pos(sent_dp[\"words\"][inner_arc[\"start\"]]['text']), \"Preposition\": sent_dp[\"words\"][inner_arc[\"end\"]]['text']}\n",
    "        else:\n",
    "          return {\"Predicate\": sent_dp[\"words\"][predicate_index]['text'], \"POS\": self.find_pos(sent_dp[\"words\"][predicate_index]['text']), \"Preposition\": None}\n",
    "    return None\n",
    "\n",
    "  def confirm_sbar(self, sent_cp: str) -> bool:\n",
    "    if \"SBAR\" in sent_cp:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "  def get_dependecy_parse(self, sent: str) -> str:\n",
    "    return displacy.parse_deps(sent)\n",
    "\n",
    "  def get_constitency_parse(self, sent: str) -> str:\n",
    "    return sent._.parse_string.replace('(', '[').replace(')', ']')\n",
    "\n",
    "  def check_children(self, sent: str) -> bool:\n",
    "      if \n",
    "      \n",
    "  def get_embedded_clauses(self, examples) -> List[dict]:\n",
    "    all_clauses = []\n",
    "    doc = nlp(examples)\n",
    "    for sent in doc.sents:\n",
    "      # print(\" \")\n",
    "      # print(sent)\n",
    "      sent_cp = self.get_constitency_parse(sent)\n",
    "      if self.confirm_sbar(sent_cp) is False:\n",
    "        continue\n",
    "      if self.check_cildren(sent):\n",
    "          # Implement recursive search\n",
    "      sent_dp = self.get_dependecy_parse(sent)\n",
    "      predicate = self.find_predicate(sent_dp)\n",
    "      if predicate == None:\n",
    "        continue\n",
    "      if self.confirm_mark(sent_dp) == True:\n",
    "        clause, clause_type = self.find_clause_with_mark(sent_dp, sent)\n",
    "      elif self.confirm_mark(sent_dp) == False:\n",
    "        clause_type = \"declarative\"\n",
    "        clause = self.find_clause_without_mark(sent_dp)\n",
    "      all_clauses.append({\"sentence\": str(sent), \"predicate\": predicate, \"clause type\": clause_type, \"clause\": clause})\n",
    "    return all_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e5658-b90f-4a39-a96f-dec62a6f5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ClauseParser()\n",
    "\n",
    "parsed_examples = parser.get_embedded_clauses(all_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fb1dc-dbd9-4ef8-a46d-9532743e3386",
   "metadata": {},
   "source": [
    "Save the parsed examples to run stats later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a19381-f37e-4a77-800a-8b80d0936b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../Parsed_Data/parsed_examples.json\"\n",
    "# Open the file in write mode\n",
    "with open(filename, \"w\") as file:\n",
    "    # Pass the file object to json.dump()\n",
    "    json.dump(parsed_examples, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39306dd-484f-4f49-bbef-69a93516939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Import dataset\n",
    "##\n",
    "#common_crawl_sample = open('../Datasets/cc_en_head-0000_sample.txt').read().replace('\\n', ' ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
