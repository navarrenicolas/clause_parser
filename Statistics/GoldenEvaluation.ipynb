{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a1a894-5b0f-47bd-af5b-d599703f5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from FullParser.ClauseParser import ClauseParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4261546-559f-4063-a1e2-cc564ad96a18",
   "metadata": {},
   "source": [
    "## Parsing tools and data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013b9739-86d3-413a-b246-5ab32e4aed54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "parser = ClauseParser()\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "# Function for quick sentence processing\n",
    "def nlp_sents(string):\n",
    "    return list(nlp(string).sents)\n",
    "\n",
    "# Golden Data file paths\n",
    "dec_path_golden = \"../Annotation/declarative_golden_set.json\"\n",
    "pol_path_golden = \".../Annotation/polar_golden_set.json\"\n",
    "alt_path_golden = \"../Annotation/alternative_golden_set.json\"\n",
    "const_path_golden = \"../Annotation/constituent_golden_set.json\"\n",
    "adv_path_golden = \"../Annotation/adversarials_golden_set.json\"\n",
    "flat_path_golden = \"../Annotation/golden_sets_flattened.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bc2859-7f68-45b4-8d92-c09b70c45c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_test = pd.read_json(adv_path_golden, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1b370-237f-47a8-9992-5d766a68bba9",
   "metadata": {},
   "source": [
    "## Parse Golden sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9883d040-97b4-4691-a344-07bccaa2a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def parse_flat_golden(filename:str):\n",
    "    golden_df = pd.read_json(filename, orient = 'index')\n",
    "    golden_parses = []\n",
    "    parser_parses = []\n",
    "    for sent in golden_df.sentence.value_counts().to_dict().keys() :\n",
    "        parses = [dict(row) for i,row in (golden_df[golden_df.sentence == sent]).iterrows()]\n",
    "        golden_parses.append(parses)\n",
    "        sent_doc = nlp(sent)\n",
    "        parsed_sent = list(sent_doc.sents)[0]\n",
    "        parser_parses.append(parser.parse_clauses(parsed_sent))\n",
    "    return (parser_parses, golden_parses)\n",
    "\n",
    "flat_parsed, flat_golden = parse_flat_golden(flat_path_golden)\n",
    "adv_parsed, adv_golden = parse_flat_golden(adv_path_golden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbd5ee-2cb4-40bf-a306-8344f99990d3",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63d67ff3-f32e-4e39-a295-d4548209b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_parses = sum([len(p) for p in adv_parsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93b447a9-637d-4051-8eb9-1efe53d327b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicate_string(predicate):\n",
    "    if len(predicate) ==0:\n",
    "        return ''\n",
    "    pred_string = ''\n",
    "    for item in predicate:\n",
    "        pred_string += str(item['lemma']) + ' '\n",
    "    return pred_string[:-1]\n",
    "\n",
    "\n",
    "def filter_sentences_idx(filt):\n",
    "    return [idx for idx in [i for i, e in enumerate(flat_golden) if filt(e)] ]\n",
    "\n",
    "single_idx = filter_sentences_idx(lambda x: len(x) == 1)\n",
    "multiple_idx = filter_sentences_idx(lambda x: len(x) > 1)\n",
    "\n",
    "\n",
    "\n",
    "def compare_data(parsed,golden,feature):\n",
    "    if feature=='predicate':\n",
    "        return [[any([(get_predicate_string(gp['predicate']) == get_predicate_string(e['predicate'])) for e in parsed[i]])\n",
    "                 for gp in gold]  \n",
    "                for i,gold in enumerate(golden)]\n",
    "    return [[any([gp[feature] == e[feature] for e in parsed[i]]) \n",
    "             for gp in gold]  \n",
    "            for i,gold in enumerate(golden)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8d489-e8ff-42bf-b982-a2a2b1a6e846",
   "metadata": {},
   "source": [
    "### F1-scores for clause detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb2fae38-9fc0-42bc-9a43-b0b95ac11149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(tp, fp, fn):\n",
    "  precision = tp/(tp+fp)\n",
    "  recall = tp/(tp+fn)\n",
    "  f1 = (2*precision*recall)/(precision + recall)\n",
    "  return [precision, recall, f1]\n",
    "\n",
    "def get_stats(parser,golden,subset='overall'):\n",
    "    \n",
    "    tp = 0\n",
    "    tn = len(adv_golden) - adv_parses if subset == 'overall' else 0\n",
    "    fp = adv_parses if subset == 'overall' else 0\n",
    "    fn = 0 \n",
    "    \n",
    "    for i in range(len(golden)):\n",
    "\n",
    "        \n",
    "            \n",
    "        gp = flat_golden[i]\n",
    "        bp = flat_parsed[i]\n",
    "        \n",
    "        if subset == 'single':\n",
    "            if len(gp) != 1:\n",
    "                continue\n",
    "        elif subset == 'multiple':\n",
    "            if len(gp) == 1:\n",
    "                continue\n",
    "        \n",
    "        if len(gp) == len(bp):\n",
    "            tp += len(gp)\n",
    "        elif len(gp) > len(bp):\n",
    "            fn += len(gp) - len(bp)\n",
    "            tp += len(bp)\n",
    "        elif len(gp) < len(bp):\n",
    "            fp += len(bp) - len(gp)\n",
    "            tp += len(gp)\n",
    "    \n",
    "    print(f\"---------Embedded Clause Detection, {subset} Clause F1---------\")\n",
    "    print(\"True Positives: \", tp, \"\\nFalse Positives: \", fp, \"\\nFalse Negatives: \", fn)\n",
    "    print(\"Precision, Recall, F1: \", get_f1(tp, fp, fn))\n",
    "    print(\"Accuracy: \", (tp+tn)/(tp+fp+fn+tn))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37088edb-a8dc-427c-8653-e06a0cace5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Embedded Clause Detection, single Clause F1---------\n",
      "True Positives:  357 \n",
      "False Positives:  40 \n",
      "False Negatives:  21\n",
      "Precision, Recall, F1:  [0.8992443324937027, 0.9444444444444444, 0.9212903225806451]\n",
      "Accuracy:  0.854066985645933\n",
      "\n",
      "---------Embedded Clause Detection, multiple Clause F1---------\n",
      "True Positives:  123 \n",
      "False Positives:  7 \n",
      "False Negatives:  26\n",
      "Precision, Recall, F1:  [0.9461538461538461, 0.825503355704698, 0.881720430107527]\n",
      "Accuracy:  0.7884615384615384\n",
      "\n",
      "---------Embedded Clause Detection, overall Clause F1---------\n",
      "True Positives:  480 \n",
      "False Positives:  53 \n",
      "False Negatives:  47\n",
      "Precision, Recall, F1:  [0.900562851782364, 0.9108159392789373, 0.9056603773584905]\n",
      "Accuracy:  0.8540145985401459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_stats(flat_parsed,flat_parsed,'single')\n",
    "get_stats(flat_parsed,flat_parsed,'multiple')\n",
    "get_stats(flat_parsed,flat_parsed,'overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c22b13-b281-4c3e-bd30-01daa1a4958d",
   "metadata": {},
   "source": [
    "## Overall Feature Detection Acuraccy\n",
    "\n",
    "- Clause\n",
    "- Clause type\n",
    "- Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f653e438-e222-467e-a98a-7a6170448d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentence accuracy 0.9351230425055929\n"
     ]
    }
   ],
   "source": [
    "# Failed sentence reproduction \n",
    "same_sentences = compare_data(flat_parsed,flat_golden,'sentence')\n",
    "print('Overall sentence accuracy', np.mean(list(map(any,same_sentences))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2ff59ad-746d-4f1e-ad67-dcaea65675ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall detection accuracy 0.9440715883668904\n",
      "Overall clause accuracy 0.8113348247576435\n",
      "Overall predicate accuracy 0.9002609992542877\n",
      "Overall type accuracy 0.9093959731543624\n"
     ]
    }
   ],
   "source": [
    "# Clause detection\n",
    "detected_clauses = [len(x)>0 for x in flat_parsed]\n",
    "print('Overall detection accuracy', np.mean(detected_clauses))\n",
    "# clause\n",
    "\n",
    "correct_clauses = compare_data(flat_parsed,flat_golden,'clause')\n",
    "print('Overall clause accuracy', np.mean([np.mean(clause) for clause in correct_clauses]))\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(flat_parsed,flat_golden,'predicate')\n",
    "print('Overall predicate accuracy', np.mean([np.mean(clause) for clause in correct_predicates]))\n",
    "\n",
    "# Type\n",
    "correct_types = compare_data(flat_parsed,flat_golden,'type')\n",
    "print('Overall type accuracy', np.mean([np.mean(clause) for clause in correct_types]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a34d7-850b-4d9d-aaa4-7d09c745d0c2",
   "metadata": {},
   "source": [
    "### Single Clause Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8df76831-b9b2-4b23-b58a-afa8ab28ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause accuracy 0.8306878306878307\n",
      "Type accuracy 0.9153439153439153\n",
      "Precicate accuracy 0.9126984126984127\n"
     ]
    }
   ],
   "source": [
    "parsed_single = [flat_parsed[i] for i in single_idx]\n",
    "golden_single = [flat_golden[i] for i in single_idx]\n",
    "\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses = compare_data(parsed_single,golden_single,'clause')\n",
    "print('Clause accuracy', np.mean(correct_clauses))\n",
    "\n",
    "correct_types = compare_data(parsed_single,golden_single,'type')\n",
    "print('Type accuracy', np.mean(correct_types))\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(parsed_single,golden_single,'predicate')\n",
    "print('Precicate accuracy',np.mean(correct_predicates))\n",
    "\n",
    "failed_single_predicates = [golden_single[i][0]['sentence'] for i,e in enumerate(correct_predicates) if (not e[0] and correct_clauses[i])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea9fcf-ee49-42d0-a0d1-71eb3967d624",
   "metadata": {},
   "source": [
    "### Multiple Clause Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70d873fc-71c6-4407-8d23-dedaafc63229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple-clauses clause accuracy  0.7053140096618357\n",
      "Multiple-clauses predicate accuracy 0.8321256038647342\n",
      "Multiple-type accuracy 0.8260869565217391\n"
     ]
    }
   ],
   "source": [
    "parsed_multiple = [flat_parsed[i] for i in multiple_idx]\n",
    "golden_multiple = [flat_golden[i] for i in multiple_idx]\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses = compare_data(parsed_multiple,golden_multiple,'clause')\n",
    "# print(len(correct_clauses))\n",
    "print('Multiple-clauses clause accuracy ',np.mean(list(map(np.mean,correct_clauses))))\n",
    "\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(parsed_multiple,golden_multiple,'predicate')\n",
    "print('Multiple-clauses predicate accuracy',np.mean(list(map(np.mean,correct_predicates))))\n",
    "\n",
    "\n",
    "correct_types = compare_data(parsed_multiple,golden_multiple,'type')\n",
    "print('Multiple-type accuracy', np.mean(list(map(all,correct_types))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be361408-f219-493b-9b45-dfe86c5964a1",
   "metadata": {},
   "source": [
    "### Single predicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "628bcc2f-7ca9-457f-ba6b-075496311996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection accuracy 0.8260869565217391\n",
      "clause accuracy  0.9211956521739131\n",
      "predicate accuracy 0.9211956521739131\n"
     ]
    }
   ],
   "source": [
    "single_preds_idx = filter_sentences_idx(lambda x: any([len(c['predicate']) == 1 for c in x]))\n",
    "\n",
    "parsed_single_pred = [flat_parsed[i] for i in single_preds_idx]\n",
    "golden_single_pred = [flat_golden[i] for i in single_preds_idx]\n",
    "\n",
    "# Detection\n",
    "\n",
    "detected_single_pred = [len(gold) == len(parsed_single_pred[i]) for i,gold in enumerate(golden_single_pred)]\n",
    "print('detection accuracy', np.mean(detected_single_pred))\n",
    "\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses_single_pred = compare_data(parsed_single_pred,golden_single_pred,'type')\n",
    "print('clause accuracy ',np.mean(list(map(np.mean,correct_clauses_single_pred))))\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates_single_pred = compare_data(parsed_single_pred,golden_single_pred,'predicate')\n",
    "print('predicate accuracy',np.mean(list(map(all,correct_predicates_single_pred))))\n",
    "\n",
    "\n",
    "failed_single_preds = [gp[0]['sentence'] for i,gp in enumerate(golden_single_pred) if (detected_single_pred[i] and  not any(correct_predicates_single_pred[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5f847-3445-4aca-bb4f-94d9bb6740e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Adversarial sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e1d884f-051e-4d3c-9fc7-c4049524fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = [parse for parse in adv_parsed if len(parse)>0]\n",
    "false_positive_sentences = [parse[0]['sentence'] for parse in false_positives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f28a4cc5-5cfa-4e19-b6bd-39c0a8244ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The phone call gets passed around a number of confused staff members, which adds to the hilarity of the video',\n",
       " 'And if you do not want to purchase clothing, I do have a few clothing options if you’d like to come to the studio to take a look.',\n",
       " 'March 27, 2019, Christian County, US 6: A 67 year-old Reeds Spring, MO man was killed when a Chevy Impala crossed the median and struck his Harley Davidson motorcycle, two other motorcyclist were seriously injured as one was also hit head-on and another motorcyclist was struck by debris',\n",
       " 'March 25, 2019; Stone County, MO 143: An 82 year-old Crane, MO woman was killed when her Chevy HHR was struck head-on by a Ford Ranger driven by a 68 year-old Marionville, MO man',\n",
       " 'February 17, 2019, Adair County, Hwy 6: A 34 year-old Brashear, MO man was seriously injured along with his 10 year-old son when a 28 year-old Canton, MO man crossed the center line on icy highway and struck their Jeep head-on',\n",
       " 'Louis, MO woman and an 18 year-old Hannibal, MO woman were both seriously injured when a Toyota Matrix driven by an 18 year-old Country Club Hills, IL man crossed the center line and struck their Mazda.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5f010cf-26d9-4000-a504-140a0a9643a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = sum(detected_clauses)/(sum(detected_clauses)+len(false_positive_sentences))\n",
    "recall = sum(detected_clauses)/(sum(detected_clauses)+447-sum(detected_clauses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86bbe2d9-066b-441a-a799-38364129fed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.985981308411215, 0.9440715883668904, 0.9645714285714286)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(precision,recall, 2*precision*recall/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505887b0-83c5-4669-9624-2edc15550685",
   "metadata": {},
   "source": [
    "# Failure Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cb6c0e8-ee36-4a96-b160-2dd95992c525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It focuses mainly on Megan and how she’s thinking and feeling, and what she’s going to do, and less so on what happens to her physically, but it was good!',\n",
       " 'Through those conversations, everything was explained bout what Megan’s options were; keeping the baby or having it adopted, and what either choice would mean for her as she is 15.',\n",
       " 'Jessica: And so it is this idea of just accepting who you are as a person and what you’re like as a person.',\n",
       " \"Whether Vengeance will include Sam Loeb's #26, and how DC will collect Mark Verheiden's issues after Jeph Loeb departs, remains to be seen.\",\n",
       " 'The economy may be improving slightly but, perhaps more importantly, the other guys are talking about restricting the accessibility of birth control or whether or not women should be in the workplace or the extent to which the devil has infiltrated American institutions among other crazy things.',\n",
       " 'Therefore, those who have not received the overtime pay they are entitled to may want to learn more about whether they are able to recover that compensation through a wage and hour claim.',\n",
       " 'So whether or not we have to be singing in order to hear this, or whether it is just going to manifest, I really do not know, but glory to God.',\n",
       " 'This includes the rights to property, the ability to raise children, and whether you will pay or receive alimony.',\n",
       " 'LONDON (AP) \\x97 British Prime Minister Theresa May is hunkered down with close allies as she considers whether to give in to relentless pressure to resign, or fight on to save her Brexit plan and her premiership.',\n",
       " 'Ultimately, whether to bootstrap or finance depends on all of the factors identified in this article, if not more.',\n",
       " \"Hyperion, it was awesome, but a warning that the first few pages are pretty lack-lustre and cliche sci-fi-ish so I nearly gave up on it (and friends have had the same experience) — but once it hits the first tale (the priest's tale) it starts to get really intriguing and compelling and not cliche ... So I'd suggest waiting till you get into that before deciding whether to persist.\",\n",
       " 'Whether, in a case of such collaboration between state and federal officers, the defendant could successfully assert his privilege in the state proceeding, we need not now decide, for the record before us is barren of evidence that the State was used as an instrument of federal prosecution or investigation.',\n",
       " 'Whether a neo-con or a progressive acts in the theater of globalization is ultimately irrelevant.',\n",
       " 'Whether a molecular configuration is designated E or Z is determined by the Cahn-Ingold-Prelog priority rules; higher atomic numbers are given higher priority.',\n",
       " '(Whether the management (M) answered the question or the works council (WC) is illustrated in brackets.)',\n",
       " 'Whether a person is ‘male’ or ‘female’ is not defined by testosterone levels, and there are no non-arbitrary definitions of testosterone levels that would make a person one of the other.',\n",
       " 'Whether the ‘drop’ of the record reverberates on a micro-scale, or to Grand Canyon echo-type proportions, you can never know.',\n",
       " 'Introducing the bill, Law Minister Ravi Shankar Prasad said despite the Supreme Court striking down the practice of talaq-e-biddat (instant triple talaq) as unconstitutional, men were divorcing their wives on flimsy grounds and even via Whatsapp.',\n",
       " 'In this sense, Mignolo argues that while modernity is not strictly a European phenomenon, its rhetoric -as Dussel argues- is formed by European philosophers, academics and politicians',\n",
       " \"Whether the factory dumps each day is random (so not all factories are active every day, in fact, if all factories were active every day, we couldn't tease apart their signals).\",\n",
       " 'Whether or not it casts those who were directly responsible in a negative light is irrelevant and, frankly, quite insignificant to the larger meaning attributed to the stories.',\n",
       " 'We are happy to discuss your case and how we can help you.',\n",
       " 'However, it’s important to discuss the way In the Future came to be and how the process impacts the album.',\n",
       " 'Delving into history, I hear the refrain, “BREAD and CIRCUSES” The Roman emperors kept the Roman mob satisfied with bread and circuses, food and entertainment, (now you understand why the liberal elite controls Hollywood,) while looting the empire to pay for the bread and the circuses, and to quiet the mob.',\n",
       " 'Query why, exactly, we allow a tax deduction for charitable giving.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_detects = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if not detected_clauses[i]]\n",
    "failed_clauses = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if (detected_clauses[i] and  not any(correct_clauses[i]) and all(correct_predicates[i]))] \n",
    "failed_preds = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if (all(same_sentences[i]) and detected_clauses[i] and  not any(correct_predicates[i]))]\n",
    "failed_detects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a2a99-91c4-47af-8573-60f2b68f1518",
   "metadata": {},
   "source": [
    "### Some useful fonctions for probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c0f693-0696-4907-9c84-0a211ed9ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_brackets(parsed_string):\n",
    "    # Replace round brackets with square brackets\n",
    "    replaced_string = parsed_string.replace('(', '[').replace(')', ']')\n",
    "    # Wrap the string with \\begin{forest} and \\end{forest}\n",
    "    final_string = '\\\\begin{adjustbox}{width=0.8\\\\linewidth}' + '\\\\begin{forest} ' + replaced_string + ' \\\\end{forest}' + '\\\\end{adjustbox}\\\\\\\\'\n",
    "    return final_string\n",
    "\n",
    "def copy_latex_parse(sentence):\n",
    "    ps = list(nlp(sentence).sents)[0]\n",
    "    return replace_brackets(ps._.parse_string)\n",
    "\n",
    "# Find parse of sentences matching keywords\n",
    "def find_parse(string):\n",
    "    return [parse for parse in flat_parsed if (lambda x: (string in x[0]['sentence']) if len(x) > 0 else False)(parse)]\n",
    "# Find golden parse of sentences matching keywords\n",
    "def find_golden_parse(string):\n",
    "    return [gp for gp in flat_golden if (string in gp[0]['sentence'])]\n",
    "\n",
    "import nltk\n",
    "def nlp_parse(sent):\n",
    "    return nltk.Tree.fromstring(list(nlp(sent).sents)[0]._.parse_string).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "123a4308-c3b7-4ee2-94e1-d01a6d2f8787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                          S                                                                         \n",
      "              ____________________________________________________________________________|_______________________________________________________________________   \n",
      "             |                                  |         S                                                                                                       | \n",
      "             |                                  |     ____|_________________                                                                                      |  \n",
      "             |                                  |    |                      VP                                                                                    | \n",
      "             |                                  |    |     _________________|_____________                                                                        |  \n",
      "             |                                  |    |    |                               VP                                                                      | \n",
      "             |                                  |    |    |         ______________________|_____________________                                                  |  \n",
      "             |                                  |    |    |        |                                            PP                                                | \n",
      "             |                                  |    |    |        |                   _________________________|_____                                            |  \n",
      "             |                                  |    |    |        |                  |               |   |          SBAR                                         | \n",
      "             |                                  |    |    |        |                  |               |   |      _____|____                                       |  \n",
      "             S                                  |    |    |        |                  |               |   |     |          S                                      | \n",
      "   __________|__________                        |    |    |        |                  |               |   |     |      ____|____                                  |  \n",
      "  |                     VP                      |    |    |        |                  |               |   |     |     |         VP                                | \n",
      "  |     ________________|____                   |    |    |        |                  |               |   |     |     |     ____|______                           |  \n",
      "  |    |                     NP                 |    |    |        |                  |               |   |     |     |    |           VP                         | \n",
      "  |    |            _________|_______           |    |    |        |                  |               |   |     |     |    |     ______|_____                     |  \n",
      "  |    |           |                 PP         |    |    |        |                  PP              |   |     |     |    |    |            VP                   | \n",
      "  |    |           |              ___|____      |    |    |        |         _________|___            |   |     |     |    |    |       _____|________            |  \n",
      "  NP   |           NP            |        NP    |    NP   |        |        |             NP          |   |     |     NP   |    |      VP    |        VP          | \n",
      "  |    |      _____|____         |        |     |    |    |        |        |     ________|_____      |   |     |     |    |    |      |     |    ____|_____      |  \n",
      "  EX  VBP    JJ        NNS       IN      NNS    CC   DT  VBP      VBN       IN  PRP$      NN    NN    CC  CC    IN   PRP  VBP  VBN    VBN    CC  NN        VBN    . \n",
      "  |    |     |          |        |        |     |    |    |        |        |    |        |     |     |   |     |     |    |    |      |     |   |          |     |  \n",
      "There are various     types      of     creams and these are distinguished  by their     fat content and  or whether they have been whipped  or heat     treated  . \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nlp_parse(find_parse('are distinguished')[0][0]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b725f09a-ea70-40f2-af09-bfc03ce1adbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At the post-secondary level, students fall into four categories, depending on whether their work placement is paid or unpaid, and whether their work placement is optional or a mandatory requirement for graduation.',\n",
       " \"Doctors and medical malpractice insurance companies currently must live with this 'bad outcome' ruling, although it is often unknown whether a better outcome was possible.\",\n",
       " \"I still get confused as to whether I'm being selfish and in typing this comment I can actually feel myself looking for affirmation from you and other posters.\",\n",
       " 'Multiplayer will include objectives that will differ depending on whether a player has chosen to fight for the Allies or Axis.',\n",
       " 'That said, it matters little whether I am over-eating or under-eating.',\n",
       " 'It is unclear to us whether the word \"new\" in this measure applies only to MOUs or whether it also applies to other pension contracts or agreements, such as those delineated in statute or those applicable to managers and supervisors.',\n",
       " 'It seems to be random whether these questions are relatively easy or impossibly difficult.',\n",
       " 'The other two cats stood poised nearby, as if undecided on whether to back up their fellow or choose the better part of valor.',\n",
       " 'There are various types of creams and these are distinguished by their fat content and or whether they have been whipped or heat treated.',\n",
       " 'The distance will be positive or negative depending on whether the point is outside or inside the circle, but this does not matter since the value is squared as part of the minimization process.',\n",
       " 'The Board members who were aware of the article should have inquired further about Sandusky and the possible risks of litigation or public relations issues, and, most importantly, whether the University has effective policies in place to protect children on its campuses.',\n",
       " 'Al Hamili remained non-committal on whether or not Opec would increase output in view of the of the current market volatility.',\n",
       " \"We're not sure whether pollsters were referring to Los Stop's version, Con su blanca palidez, though somehow we suspect not.\",\n",
       " 'Different weights are assigned to the words based on whether they appear in the title, subheadings or the body of the page and the number of times a particular word appears in the webpage is stored.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1951062a-8280-4784-b9a5-b1392df3384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence The other two cats stood poised nearby, as if undecided on whether to back up their fellow or choose the better part of valor. \n",
      "\n",
      "Golden [([{'str': 'undecided', 'lemma': 'undecided', 'POS': 'ADJ'}, {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}], 'whether to back up their fellow or choose the better part of valor')]\n",
      "\n",
      "\n",
      "Parsed [([{'str': 'stood', 'lemma': 'stand', 'POS': 'VERB'}, {'str': 'poised', 'lemma': 'poise', 'POS': 'VERB'}, {'str': 'undecided', 'lemma': 'undecided', 'POS': 'ADJ'}, {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}], 'whether to back up their fellow or choose the better part of valor')]\n",
      "                                                                      S                                                                               \n",
      "       _______________________________________________________________|_____________________________________________________________________________   \n",
      "      |                           VP                                                                                                                | \n",
      "      |               ____________|_____________                                                                                                    |  \n",
      "      |              |     |      |     |      SBAR                                                                                                 | \n",
      "      |              |     |      |     |    ___|______________                                                                                     |  \n",
      "      |              |     |      |     |   |   |             ADJP                                                                                  | \n",
      "      |              |     |      |     |   |   |        ______|____________                                                                        |  \n",
      "      |              |     |      |     |   |   |       |                   PP                                                                      | \n",
      "      |              |     |      |     |   |   |       |       ____________|________                                                               |  \n",
      "      |              |     |      |     |   |   |       |      |                    SBAR                                                            | \n",
      "      |              |     |      |     |   |   |       |      |       ______________|_____________________                                         |  \n",
      "      |              |     |      |     |   |   |       |      |      |                                    S                                        | \n",
      "      |              |     |      |     |   |   |       |      |      |                                    |                                        |  \n",
      "      |              |     |      |     |   |   |       |      |      |                                    VP                                       | \n",
      "      |              |     |      |     |   |   |       |      |      |      ______________________________|____                                    |  \n",
      "      |              |     |      |     |   |   |       |      |      |     |                                   VP                                  | \n",
      "      |              |     |      |     |   |   |       |      |      |     |         __________________________|__________                         |  \n",
      "      |              |     |      |     |   |   |       |      |      |     |        |                     |               VP                       | \n",
      "      |              |     |      |     |   |   |       |      |      |     |        |                     |     __________|_____                   |  \n",
      "      |              |     |      |     |   |   |       |      |      |     |        |                     |    |                NP                 | \n",
      "      |              |     |      |     |   |   |       |      |      |     |        |                     |    |           _____|________          |  \n",
      "      |              |     |      |     |   |   |       |      |      |     |        VP                    |    |          |              PP        | \n",
      "      |              |     |      |     |   |   |       |      |      |     |    ____|__________           |    |          |           ___|____     |  \n",
      "      NP             |     |     ADVP   |   |   |       |      |      |     |   |   PRT         NP         |    |          NP         |        NP   | \n",
      "  ____|________      |     |      |     |   |   |       |      |      |     |   |    |      ____|____      |    |      ____|_____     |        |    |  \n",
      " DT   JJ   CD NNS   VBD   VBN     RB    ,   IN  IN      JJ     IN     IN    TO  VB   RP   PRP$       NN    CC   VB    DT  JJR    NN   IN       NN   . \n",
      " |    |    |   |     |     |      |     |   |   |       |      |      |     |   |    |     |         |     |    |     |    |     |    |        |    |  \n",
      "The other two cats stood poised nearby  ,   as  if  undecided  on  whether  to back  up  their     fellow  or choose the better part  of     valor  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the golden parses matching query\n",
    "fail_idx = 7\n",
    "fail_sentences = failed_preds\n",
    "print('sentence', fail_sentences[fail_idx],'\\n')\n",
    "\n",
    "print('Golden',[(gp['predicate'], gp['clause']) for gp in find_golden_parse(fail_sentences[fail_idx])[0]])\n",
    "print('\\n')\n",
    "print('Parsed',[(p['predicate'],p['clause']) for p in find_parse(fail_sentences[fail_idx])[0]])\n",
    "\n",
    "# parser.parse_clauses(nlp_sents(failed_preds[fail_idx])[0])\n",
    "nlp_parse(fail_sentences[fail_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63382c68-aea9-42b3-9341-5a1881ef3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pyperclip \n",
    "\n",
    "trees = ''\n",
    "for sentence in failed_preds:\n",
    "    trees += replace_brackets(list(nlp(sentence).sents)[0]._.parse_string) + '\\n' \n",
    "pyperclip.copy(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77844ba1-5db0-45f7-8894-65dfc9a54e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "know          58\n",
       "decide        30\n",
       "say           26\n",
       "ask           21\n",
       "think         15\n",
       "determine     14\n",
       "tell          14\n",
       "wonder        13\n",
       "see           12\n",
       "understand    10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_clauses = []\n",
    "for gold in flat_golden:\n",
    "    for gp in gold:\n",
    "        gold_clauses.append(gp['clause'])\n",
    "\n",
    "# Quick reflection of the golden single predicates\n",
    "gold_predicates = []\n",
    "for gold in flat_golden:\n",
    "    for gp in gold:\n",
    "        gold_predicates.append(gp['predicate'])\n",
    "gold_single_preds = pd.Series([pred[0]['lemma'] for pred in  filter(lambda x: len(x)==1,gold_predicates)])\n",
    "gold_single_preds.value_counts()[0:10]\n",
    "\n",
    "\n",
    "[pred for pred in gold_predicates if  len(pred)>2]\n",
    "\n",
    "# Quick reflection of the parsed single predicates\n",
    "parsed_predicates = []\n",
    "for sent_parse in flat_parsed:\n",
    "    for parse in sent_parse:\n",
    "        parsed_predicates.append(parse['predicate'])\n",
    "parsed_single_preds = pd.Series([pred[0]['lemma'] for pred in  filter(lambda x: len(x)==1,parsed_predicates)])\n",
    "parsed_single_preds.value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c062c62c-0adb-468a-b3f9-c4fa2113c9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'m aware of\",\n",
       " \"'m sure\",\n",
       " \"'re curious\",\n",
       " \"'re sure\",\n",
       " \"'s clear\",\n",
       " \"'s telling\",\n",
       " 'Claims',\n",
       " 'Deciding',\n",
       " 'Find out',\n",
       " 'Measuring',\n",
       " 'Query',\n",
       " 'Wonder',\n",
       " 'accepting',\n",
       " 'addressing',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitting',\n",
       " 'agreed',\n",
       " 'analyzes',\n",
       " 'analyzing',\n",
       " 'announced',\n",
       " 'apologize for',\n",
       " 'appreciate',\n",
       " 'are sure',\n",
       " 'are unclear as to',\n",
       " 'argue',\n",
       " 'argues',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asked about',\n",
       " 'asking',\n",
       " 'assess',\n",
       " 'based on',\n",
       " 'be irrelevant',\n",
       " 'be random',\n",
       " 'believe',\n",
       " 'believes',\n",
       " 'capture',\n",
       " 'care',\n",
       " 'caring about',\n",
       " 'check',\n",
       " 'choosing',\n",
       " 'claims',\n",
       " 'clarify',\n",
       " 'comment on',\n",
       " 'concerns',\n",
       " 'conclude',\n",
       " 'configure',\n",
       " 'confirm',\n",
       " 'confused as to',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'considers',\n",
       " 'control',\n",
       " 'debating over',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decides',\n",
       " 'deciding',\n",
       " 'declaring',\n",
       " 'defined',\n",
       " 'demonstrated',\n",
       " 'demonstrates',\n",
       " 'denies',\n",
       " 'deny',\n",
       " 'depend on',\n",
       " 'depending on',\n",
       " 'depends on',\n",
       " 'determine',\n",
       " 'determining',\n",
       " 'dictate',\n",
       " 'discern',\n",
       " 'discover',\n",
       " 'discuss',\n",
       " 'discusses',\n",
       " 'distinguish',\n",
       " 'distinguished by',\n",
       " 'doubt',\n",
       " 'ensure',\n",
       " 'ensured',\n",
       " 'establish',\n",
       " 'estimate',\n",
       " 'estimates',\n",
       " 'evaluate',\n",
       " 'examine',\n",
       " 'explain',\n",
       " 'explained',\n",
       " 'explained bout',\n",
       " 'explaining',\n",
       " 'explains',\n",
       " 'fear',\n",
       " 'feel',\n",
       " 'felt',\n",
       " 'fighting about',\n",
       " 'fighting over',\n",
       " 'figure out',\n",
       " 'find',\n",
       " 'find out',\n",
       " 'flesh out',\n",
       " 'focuses on',\n",
       " 'found',\n",
       " 'get',\n",
       " 'gone on',\n",
       " 'guarantee',\n",
       " 'guess',\n",
       " 'hear',\n",
       " 'hear of',\n",
       " 'hearing',\n",
       " 'held',\n",
       " 'hesitant as to',\n",
       " 'hinge on',\n",
       " 'holds',\n",
       " 'honest are about',\n",
       " 'hope',\n",
       " 'hopes',\n",
       " 'identify',\n",
       " 'illustrated',\n",
       " 'imagine',\n",
       " 'includes',\n",
       " 'indicate',\n",
       " 'influence',\n",
       " 'inform',\n",
       " 'inquired about',\n",
       " 'investigate',\n",
       " 'investigating',\n",
       " 'is aware',\n",
       " 'is certain',\n",
       " 'is clear',\n",
       " 'is confusing',\n",
       " 'is dependent on',\n",
       " 'is doubtful',\n",
       " 'is immaterial',\n",
       " 'is irrelevant',\n",
       " 'is random',\n",
       " 'is unclear',\n",
       " 'is unknown',\n",
       " 'is unkown',\n",
       " 'is wonderful',\n",
       " 'judge',\n",
       " 'know',\n",
       " 'knowing',\n",
       " 'known',\n",
       " 'known about',\n",
       " 'knows',\n",
       " 'learn',\n",
       " 'learn about',\n",
       " 'learned',\n",
       " 'look into',\n",
       " 'make',\n",
       " 'make sure',\n",
       " 'matter',\n",
       " 'matters',\n",
       " 'means',\n",
       " 'measure',\n",
       " 'mention',\n",
       " 'monitor',\n",
       " 'mused about',\n",
       " 'note',\n",
       " 'noted',\n",
       " 'noticed',\n",
       " 'pestering about',\n",
       " 'praying',\n",
       " 'predict',\n",
       " 'probe',\n",
       " 'puzzled by',\n",
       " 'question',\n",
       " 'real are about',\n",
       " 'realize',\n",
       " 'realized',\n",
       " 'recall',\n",
       " 'recognize',\n",
       " 'reflect',\n",
       " 'reflect on',\n",
       " 'reiterate',\n",
       " 'relates',\n",
       " 'remained non-committal on',\n",
       " 'remains ambiguous',\n",
       " 'remains unclear',\n",
       " 'remember',\n",
       " 'remembers',\n",
       " 'reminded',\n",
       " 'report',\n",
       " 'reported',\n",
       " 'reports',\n",
       " 'researching',\n",
       " 'reveal',\n",
       " 'revealed',\n",
       " 'reveals',\n",
       " 'said',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'scout',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seen',\n",
       " 'show',\n",
       " 'signaling',\n",
       " 'speak to',\n",
       " 'speculating',\n",
       " 'state',\n",
       " 'suggesting',\n",
       " 'talk about',\n",
       " 'talking about',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'testing',\n",
       " 'think',\n",
       " 'think about',\n",
       " 'think of',\n",
       " 'thinking',\n",
       " 'thinking about',\n",
       " 'thought',\n",
       " 'told',\n",
       " 'tweeted',\n",
       " 'undecided on',\n",
       " 'understand',\n",
       " 'understands',\n",
       " 'understood',\n",
       " 'was stunned',\n",
       " 'wish',\n",
       " 'wonder',\n",
       " 'wondered',\n",
       " 'wondering',\n",
       " 'wondering about',\n",
       " 'wonders',\n",
       " 'worry about',\n",
       " 'worrying',\n",
       " 'write'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([' '.join(p['str'] for p in pred) for pred in gold_predicates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2e91389-f634-4a36-a074-a8cba6a4ea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'str': 'says', 'lemma': 'say', 'POS': 'VERB'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'AUX'},\n",
       "  {'str': 'enjoyed', 'lemma': 'enjoy', 'POS': 'VERB'},\n",
       "  {'str': 'working', 'lemma': 'work', 'POS': 'VERB'},\n",
       "  {'str': 'for', 'lemma': 'for', 'POS': 'ADP'}],\n",
       " [{'str': \"'s\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'impossible', 'lemma': 'impossible', 'POS': 'ADJ'},\n",
       "  {'str': 'find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'half', 'lemma': 'half', 'POS': 'ADJ'},\n",
       "  {'str': 'is', 'lemma': 'be', 'POS': 'AUX'}],\n",
       " [{'str': 'provided', 'lemma': 'provide', 'POS': 'VERB'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'},\n",
       "  {'str': 'should', 'lemma': 'should', 'POS': 'AUX'},\n",
       "  {'str': 'charge', 'lemma': 'charge', 'POS': 'VERB'}],\n",
       " [{'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'get', 'lemma': 'get', 'POS': 'VERB'},\n",
       "  {'str': 'up', 'lemma': 'up', 'POS': 'ADP'},\n",
       "  {'str': 'go', 'lemma': 'go', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': 'be', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'positive', 'lemma': 'positive', 'POS': 'ADJ'},\n",
       "  {'str': 'negative', 'lemma': 'negative', 'POS': 'ADJ'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'inquired', 'lemma': 'inquire', 'POS': 'VERB'},\n",
       "  {'str': 'about', 'lemma': 'about', 'POS': 'ADP'},\n",
       "  {'str': 'possible', 'lemma': 'possible', 'POS': 'ADJ'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'}],\n",
       " [{'str': 'remained', 'lemma': 'remain', 'POS': 'VERB'},\n",
       "  {'str': 'non', 'lemma': 'non', 'POS': 'ADJ'},\n",
       "  {'str': '-', 'lemma': '-', 'POS': 'ADJ'},\n",
       "  {'str': 'committal', 'lemma': 'committal', 'POS': 'ADJ'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'sure', 'lemma': 'sure', 'POS': 'ADJ'},\n",
       "  {'str': 'were', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'referring', 'lemma': 'refer', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'sure', 'lemma': 'sure', 'POS': 'ADJ'},\n",
       "  {'str': 'were', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'referring', 'lemma': 'refer', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': \"'m\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'interested', 'lemma': 'interested', 'POS': 'ADJ'},\n",
       "  {'str': 'in', 'lemma': 'in', 'POS': 'ADP'},\n",
       "  {'str': 'spammy', 'lemma': 'spammy', 'POS': 'ADJ'},\n",
       "  {'str': 'commercial', 'lemma': 'commercial', 'POS': 'ADJ'},\n",
       "  {'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'proposing', 'lemma': 'propose', 'POS': 'VERB'}],\n",
       " [{'str': 'Find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'contacting', 'lemma': 'contact', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsed predicates with more than 4 items (many more than the golden set)\n",
    "list(pred for pred in parsed_predicates if len(pred)>4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "753c0c86-aa84-4c25-a033-8df59da32d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'str': 'says', 'lemma': 'say', 'POS': 'VERB'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'AUX'},\n",
       "  {'str': 'enjoyed', 'lemma': 'enjoy', 'POS': 'VERB'},\n",
       "  {'str': 'working', 'lemma': 'work', 'POS': 'VERB'},\n",
       "  {'str': 'for', 'lemma': 'for', 'POS': 'ADP'}],\n",
       " [{'str': 'comment', 'lemma': 'comment', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'planning', 'lemma': 'plan', 'POS': 'VERB'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'provided', 'lemma': 'provide', 'POS': 'VERB'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'},\n",
       "  {'str': 'should', 'lemma': 'should', 'POS': 'AUX'},\n",
       "  {'str': 'charge', 'lemma': 'charge', 'POS': 'VERB'}],\n",
       " [{'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'get', 'lemma': 'get', 'POS': 'VERB'},\n",
       "  {'str': 'up', 'lemma': 'up', 'POS': 'ADP'},\n",
       "  {'str': 'go', 'lemma': 'go', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': 'stopped', 'lemma': 'stop', 'POS': 'VERB'},\n",
       "  {'str': 'wondered', 'lemma': 'wonder', 'POS': 'VERB'},\n",
       "  {'str': 'kill', 'lemma': 'kill', 'POS': 'VERB'}],\n",
       " [{'str': 'tell', 'lemma': 'tell', 'POS': 'VERB'},\n",
       "  {'str': 'before', 'lemma': 'before', 'POS': 'ADP'},\n",
       "  {'str': 'merging', 'lemma': 'merge', 'POS': 'VERB'}],\n",
       " [{'str': 'differ', 'lemma': 'differ', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'seated', 'lemma': 'seat', 'POS': 'VERB'},\n",
       "  {'str': 'lying', 'lemma': 'lie', 'POS': 'VERB'},\n",
       "  {'str': 'down', 'lemma': 'down', 'POS': 'ADP'}],\n",
       " [{'str': 'stood', 'lemma': 'stand', 'POS': 'VERB'},\n",
       "  {'str': 'poised', 'lemma': 'poise', 'POS': 'VERB'}],\n",
       " [{'str': 'assigned', 'lemma': 'assign', 'POS': 'VERB'},\n",
       "  {'str': 'based', 'lemma': 'base', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'assigned', 'lemma': 'assign', 'POS': 'VERB'},\n",
       "  {'str': 'based', 'lemma': 'base', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'has', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'},\n",
       "  {'str': 'enthuse', 'lemma': 'enthuse', 'POS': 'VERB'}],\n",
       " [{'str': 'know', 'lemma': 'know', 'POS': 'VERB'},\n",
       "  {'str': 'Chinese', 'lemma': 'chinese', 'POS': 'ADJ'},\n",
       "  {'str': 'central', 'lemma': 'central', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'VERB'}],\n",
       " [{'str': 'know', 'lemma': 'know', 'POS': 'VERB'},\n",
       "  {'str': 'Chinese', 'lemma': 'chinese', 'POS': 'ADJ'},\n",
       "  {'str': 'central', 'lemma': 'central', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'VERB'}],\n",
       " [{'str': 'asked', 'lemma': 'ask', 'POS': 'VERB'},\n",
       "  {'str': 'about', 'lemma': 'about', 'POS': 'ADP'},\n",
       "  {'str': 'grown', 'lemma': 'grow', 'POS': 'VERB'}],\n",
       " [{'str': 'Find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'contacting', 'lemma': 'contact', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsed predicates with more then one verb per embedding predicate (more than in the golden set)\n",
    "list(pred for pred in parsed_predicates if (lambda x: len([pr for pr in pred if pr['POS'] == 'VERB'])>1)(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c293623-611f-4295-bbd0-404b9b9ee25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benepar_env",
   "language": "python",
   "name": "benepar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
