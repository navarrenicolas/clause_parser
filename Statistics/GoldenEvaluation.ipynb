{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a1a894-5b0f-47bd-af5b-d599703f5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from FullParser.ClauseParser import ClauseParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4261546-559f-4063-a1e2-cc564ad96a18",
   "metadata": {},
   "source": [
    "## Parsing tools and data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013b9739-86d3-413a-b246-5ab32e4aed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ClauseParser()\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "# Function for quick sentence processing\n",
    "def nlp_sents(string):\n",
    "    return list(nlp(string).sents)\n",
    "\n",
    "# Golden Data file paths\n",
    "dec_path_golden = \"../Annotation/declarative_golden_set.json\"\n",
    "pol_path_golden = \".../Annotation/polar_golden_set.json\"\n",
    "alt_path_golden = \"../Annotation/alternative_golden_set.json\"\n",
    "const_path_golden = \"../Annotation/constituent_golden_set.json\"\n",
    "adv_path_golden = \"../Annotation/adversarials_golden_set.json\"\n",
    "flat_path_golden = \"../Annotation/golden_sets_flattened.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1b370-237f-47a8-9992-5d766a68bba9",
   "metadata": {},
   "source": [
    "## Parse Golden sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9883d040-97b4-4691-a344-07bccaa2a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def parse_flat_golden(filename:str):\n",
    "    golden_df = pd.read_json(filename, orient = 'index')\n",
    "    golden_parses = []\n",
    "    parser_parses = []\n",
    "    for sent in golden_df.sentence.value_counts().to_dict().keys() :\n",
    "        parses = [dict(row) for i,row in (golden_df[golden_df.sentence == sent]).iterrows()]\n",
    "        golden_parses.append(parses)\n",
    "        sent_doc = nlp(sent)\n",
    "        parsed_sent = list(sent_doc.sents)[0]\n",
    "        parser_parses.append(parser.parse_clauses(parsed_sent))\n",
    "    return (parser_parses, golden_parses)\n",
    "\n",
    "flat_parsed, flat_golden = parse_flat_golden(flat_path_golden)\n",
    "adv_parsed, adv_golden = parse_flat_golden(adv_path_golden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbd5ee-2cb4-40bf-a306-8344f99990d3",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b447a9-637d-4051-8eb9-1efe53d327b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicate_string(predicate):\n",
    "    if len(predicate) ==0:\n",
    "        return ''\n",
    "    pred_string = ''\n",
    "    for item in predicate:\n",
    "        pred_string += str(item['str']) + ' '\n",
    "    return pred_string[:-1]\n",
    "\n",
    "\n",
    "def filter_sentences_idx(filt):\n",
    "    return [idx for idx in [i for i, e in enumerate(flat_golden) if filt(e)] ]\n",
    "\n",
    "single_idx = filter_sentences_idx(lambda x: len(x) == 1)\n",
    "multiple_idx = filter_sentences_idx(lambda x: len(x) > 1)\n",
    "\n",
    "\n",
    "def compare_data(parsed,golden,feature):\n",
    "    if feature== 'type':\n",
    "        return [[any([(gp['clause'] == e['clause'] and  gp['type'] == e['type']) for e in parsed[i]]) for gp in gold]  for i,gold in enumerate(golden)]\n",
    "    if feature=='predicate':\n",
    "        return [[any([(get_predicate_string(gp['predicate']) == get_predicate_string(e['predicate'])) for e in parsed[i]]) for gp in gold]  for i,gold in enumerate(golden)]\n",
    "    return [[any([gp[feature] == e[feature] for e in parsed[i]]) for gp in gold]  for i,gold in enumerate(golden)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a34d7-850b-4d9d-aaa4-7d09c745d0c2",
   "metadata": {},
   "source": [
    "## Single Clause Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8df76831-b9b2-4b23-b58a-afa8ab28ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection accuracy 0.9523809523809523\n",
      "Clause accuracy 0.8783068783068783\n",
      "Precicate accuracy 0.8994708994708994\n",
      "Type accuracy 0.8994708994708994\n"
     ]
    }
   ],
   "source": [
    "parsed_single = [flat_parsed[i] for i in single_idx]\n",
    "golden_single = [flat_golden[i] for i in single_idx]\n",
    "\n",
    "# Clause detection\n",
    "\n",
    "detected_clauses =  list(map(lambda x: len(x)>0,parsed_single))\n",
    "print('Detection accuracy', np.mean(detected_clauses))\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses = compare_data(parsed_single,golden_single,'type')\n",
    "print('Clause accuracy', np.mean(correct_clauses))\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(parsed_single,golden_single,'predicate')\n",
    "print('Precicate accuracy',np.mean(correct_predicates))\n",
    "\n",
    "failed_single_predicates = [golden_single[i][0]['sentence'] for i,e in enumerate(correct_predicates) if (not e[0] and correct_clauses[i])]\n",
    "\n",
    "correct_types = compare_data(parsed_single,golden_single,'type')\n",
    "print('Type accuracy',np.mean(correct_predicates))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea9fcf-ee49-42d0-a0d1-71eb3967d624",
   "metadata": {},
   "source": [
    "## Multiple Clause Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70d873fc-71c6-4407-8d23-dedaafc63229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple-clauses detection accuracy 0.8783068783068783\n",
      "Multiple-clauses clause accuracy  0.8783068783068783\n",
      "Multiple-clauses predicate accuracy 0.8994708994708994\n"
     ]
    }
   ],
   "source": [
    "parsed_multiple = [flat_parsed[i] for i in single_idx]\n",
    "golden_multiple = [flat_golden[i] for i in single_idx]\n",
    "\n",
    "# Detection\n",
    "\n",
    "detected_multiple = [len(gold) == len(parsed_multiple[i]) for i,gold in enumerate(golden_multiple)]\n",
    "print('Multiple-clauses detection accuracy', np.mean(correct_clauses))\n",
    "\n",
    "failed_multiple_detect = [parsed_multiple[i] for i,detect in enumerate(detected_multiple) if not detect]\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses = compare_data(parsed_multiple,golden_multiple,'type')\n",
    "print('Multiple-clauses clause accuracy ',np.mean(list(map(np.mean,correct_clauses))))\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(parsed_multiple,golden_multiple,'predicate')\n",
    "print('Multiple-clauses predicate accuracy',np.mean(list(map(np.mean,correct_predicates))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c22b13-b281-4c3e-bd30-01daa1a4958d",
   "metadata": {},
   "source": [
    "## Overall Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2ff59ad-746d-4f1e-ad67-dcaea65675ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall detection accuracy 0.9530201342281879\n",
      "Overall clause accuracy 0.8322147651006712\n",
      "Overall predicate accuracy 0.8747203579418344\n"
     ]
    }
   ],
   "source": [
    "# Clause detection\n",
    "\n",
    "detected_clauses = list(map(lambda x: len(x)>0,flat_parsed))\n",
    "print('Overall detection accuracy', np.mean(detected_clauses))\n",
    "\n",
    "failed_detects = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if not detected_clauses[i]]\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses = compare_data(flat_parsed,flat_golden,'type')\n",
    "print('Overall clause accuracy', np.mean(list(map(all,correct_clauses))))\n",
    "\n",
    "failed_clauses = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if (detected_clauses[i] and  not any(correct_clauses[i]) and all(correct_predicates[i]))] \n",
    "\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(flat_parsed,flat_golden,'predicate')\n",
    "print('Overall predicate accuracy', np.mean(list(map(all,correct_predicates))))\n",
    "\n",
    "failed_preds = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if (detected_clauses[i] and  not any(correct_predicates[i]))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5f847-3445-4aca-bb4f-94d9bb6740e8",
   "metadata": {},
   "source": [
    "## Adversarial sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e1d884f-051e-4d3c-9fc7-c4049524fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = [parse for parse in adv_parsed if len(parse)>0]\n",
    "false_positive_sentences = [parse[0]['sentence'] for parse in false_positives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae32d7ac-2bd4-44c8-8d7a-1c23320cd043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False], [False], [True], [False], [True], [False], [False], [False], [False]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[fp['predicate'] == [] for fp in ent] for ent in false_positives]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505887b0-83c5-4669-9624-2edc15550685",
   "metadata": {},
   "source": [
    "## Failure Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a2a99-91c4-47af-8573-60f2b68f1518",
   "metadata": {},
   "source": [
    "### Some useful fonctions for probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7c0f693-0696-4907-9c84-0a211ed9ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_brackets(parsed_string):\n",
    "    # Replace round brackets with square brackets\n",
    "    replaced_string = parsed_string.replace('(', '[').replace(')', ']')\n",
    "    # Wrap the string with \\begin{forest} and \\end{forest}\n",
    "    final_string = '\\\\begin{adjustbox}{width=0.8\\\\linewidth}' + '\\\\begin{forest} ' + replaced_string + ' \\\\end{forest}' + '\\\\end{adjustbox}\\\\\\\\'\n",
    "    return final_string\n",
    "\n",
    "def copy_latex_parse(sentence):\n",
    "    ps = list(nlp(sentence).sents)[0]\n",
    "    return replace_brackets(ps._.parse_string)\n",
    "\n",
    "# Find parse of sentences matching keywords\n",
    "def find_parse(string):\n",
    "    return [parse for parse in flat_parsed if (lambda x: (string in x[0]['sentence']) if len(x) > 0 else False)(parse)]\n",
    "# Find golden parse of sentences matching keywords\n",
    "def find_golden_parse(string):\n",
    "    return [gp for gp in flat_golden if (string in gp[0]['sentence'])]\n",
    "\n",
    "import nltk\n",
    "def nlp_parse(sent):\n",
    "    return nltk.Tree.fromstring(list(nlp(sent).sents)[0]._.parse_string).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1951062a-8280-4784-b9a5-b1392df3384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden [([{'str': 'comment', 'lemma': 'comment', 'POS': 'VERB'}, {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}], 'how'), ([{'str': 'comment', 'lemma': 'comment', 'POS': 'VERB'}, {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}], 'what he is planning')]\n",
      "\n",
      "\n",
      "Parsed [([{'str': 'comment', 'lemma': 'comment', 'POS': 'VERB'}, {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}, {'str': 'planning', 'lemma': 'plan', 'POS': 'VERB'}], 'how or what he is planning')]\n",
      "                                                     S                                                          \n",
      "  ___________________________________________________|________________________________________________________   \n",
      " |   |                                    VP                                                                  | \n",
      " |   |                 ___________________|________________                                                   |  \n",
      " |   |                |                         |          VP                                                 | \n",
      " |   |                |                         |     _____|_____                                             |  \n",
      " |   |                |                         |    |           S                                            | \n",
      " |   |                |                         |    |           |                                            |  \n",
      " |   |                |                         |    |           VP                                           | \n",
      " |   |                |                         |    |      _____|_____                                       |  \n",
      " |   |                |                         |    |     |           VP                                     | \n",
      " |   |                |                         |    |     |      _____|___                                   |  \n",
      " |   |                VP                        |    |     |     |         PP                                 | \n",
      " |   |      __________|_____                    |    |     |     |      ___|________                          |  \n",
      " |   |     |                S                   |    |     |     |     |           SBAR                       | \n",
      " |   |     |                |                   |    |     |     |     |        ____|________                 |  \n",
      " |   |     |                VP                  |    |     |     |     |       |             S                | \n",
      " |   |     |     ___________|____               |    |     |     |     |       |          ___|___             |  \n",
      " |   |     |    |                VP             |    |     |     |     |       |         |       VP           | \n",
      " |   |     |    |      __________|___           |    |     |     |     |       |         |    ___|_____       |  \n",
      " NP ADVP   |    |     |              NP         |    |     |     |     |      WHNP       NP  |         VP     | \n",
      " |   |     |    |     |      ________|____      |    |     |     |     |    ___|____     |   |         |      |  \n",
      "PRP  RB   VBZ   TO    VB   PRP$      JJ   NN    CC  VBD    TO    VB    IN WRB  CC   WP  PRP VBZ       VBG     . \n",
      " |   |     |    |     |     |        |    |     |    |     |     |     |   |   |    |    |   |         |      |  \n",
      " He also plans  to develop his      own career but failed  to comment  on how  or  what  he  is     planning  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the golden parses matching query\n",
    "fail_idx = 0\n",
    "print('Golden',[(gp['predicate'], gp['clause']) for gp in find_golden_parse(failed_preds[fail_idx])[0]])\n",
    "print('\\n')\n",
    "print('Parsed',[(p['predicate'],p['clause']) for p in find_parse(failed_preds[fail_idx])[0]])\n",
    "\n",
    "parser.parse_clauses(nlp_sents(failed_preds[fail_idx])[0])\n",
    "nlp_parse(failed_preds[fail_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77844ba1-5db0-45f7-8894-65dfc9a54e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "know         76\n",
       "decide       34\n",
       "say          32\n",
       "ask          22\n",
       "tell         17\n",
       "think        17\n",
       "wonder       17\n",
       "see          15\n",
       "determine    14\n",
       "discuss      12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_clauses = []\n",
    "for gold in flat_golden:\n",
    "    for gp in gold:\n",
    "        gold_clauses.append(gp['clause'])\n",
    "\n",
    "# Quick reflection of the golden single predicates\n",
    "gold_predicates = []\n",
    "for gold in flat_golden:\n",
    "    for gp in gold:\n",
    "        gold_predicates.append(gp['predicate'])\n",
    "gold_single_preds = pd.Series([pred[0]['lemma'] for pred in  filter(lambda x: len(x)==1,gold_predicates)])\n",
    "gold_single_preds.value_counts()[0:10]\n",
    "\n",
    "\n",
    "[pred for pred in gold_predicates if  len(pred)>2]\n",
    "\n",
    "# Quick reflection of the parsed single predicates\n",
    "parsed_predicates = []\n",
    "for sent_parse in flat_parsed:\n",
    "    for parse in sent_parse:\n",
    "        parsed_predicates.append(parse['predicate'])\n",
    "parsed_single_preds = pd.Series([pred[0]['lemma'] for pred in  filter(lambda x: len(x)==1,parsed_predicates)])\n",
    "parsed_single_preds.value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2e91389-f634-4a36-a074-a8cba6a4ea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'str': 'says', 'lemma': 'say', 'POS': 'VERB'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'AUX'},\n",
       "  {'str': 'enjoyed', 'lemma': 'enjoy', 'POS': 'VERB'},\n",
       "  {'str': 'working', 'lemma': 'work', 'POS': 'VERB'},\n",
       "  {'str': 'for', 'lemma': 'for', 'POS': 'ADP'}],\n",
       " [{'str': \"'s\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'impossible', 'lemma': 'impossible', 'POS': 'ADJ'},\n",
       "  {'str': 'find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'half', 'lemma': 'half', 'POS': 'ADJ'},\n",
       "  {'str': 'is', 'lemma': 'be', 'POS': 'AUX'}],\n",
       " [{'str': 'provided', 'lemma': 'provide', 'POS': 'VERB'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'},\n",
       "  {'str': 'should', 'lemma': 'should', 'POS': 'AUX'},\n",
       "  {'str': 'charge', 'lemma': 'charge', 'POS': 'VERB'}],\n",
       " [{'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'get', 'lemma': 'get', 'POS': 'VERB'},\n",
       "  {'str': 'up', 'lemma': 'up', 'POS': 'ADP'},\n",
       "  {'str': 'go', 'lemma': 'go', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': 'be', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'positive', 'lemma': 'positive', 'POS': 'ADJ'},\n",
       "  {'str': 'negative', 'lemma': 'negative', 'POS': 'ADJ'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'inquired', 'lemma': 'inquire', 'POS': 'VERB'},\n",
       "  {'str': 'about', 'lemma': 'about', 'POS': 'ADP'},\n",
       "  {'str': 'possible', 'lemma': 'possible', 'POS': 'ADJ'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'}],\n",
       " [{'str': 'remained', 'lemma': 'remain', 'POS': 'VERB'},\n",
       "  {'str': 'non', 'lemma': 'non', 'POS': 'ADJ'},\n",
       "  {'str': '-', 'lemma': '-', 'POS': 'ADJ'},\n",
       "  {'str': 'committal', 'lemma': 'committal', 'POS': 'ADJ'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'sure', 'lemma': 'sure', 'POS': 'ADJ'},\n",
       "  {'str': 'were', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'referring', 'lemma': 'refer', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'sure', 'lemma': 'sure', 'POS': 'ADJ'},\n",
       "  {'str': 'were', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'referring', 'lemma': 'refer', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': \"'m\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'interested', 'lemma': 'interested', 'POS': 'ADJ'},\n",
       "  {'str': 'in', 'lemma': 'in', 'POS': 'ADP'},\n",
       "  {'str': 'spammy', 'lemma': 'spammy', 'POS': 'ADJ'},\n",
       "  {'str': 'commercial', 'lemma': 'commercial', 'POS': 'ADJ'},\n",
       "  {'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'proposing', 'lemma': 'propose', 'POS': 'VERB'}],\n",
       " [{'str': 'Find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'contacting', 'lemma': 'contact', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsed predicates with more than 4 items (many more than the golden set)\n",
    "list(pred for pred in parsed_predicates if len(pred)>4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "753c0c86-aa84-4c25-a033-8df59da32d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'str': 'says', 'lemma': 'say', 'POS': 'VERB'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'AUX'},\n",
       "  {'str': 'enjoyed', 'lemma': 'enjoy', 'POS': 'VERB'},\n",
       "  {'str': 'working', 'lemma': 'work', 'POS': 'VERB'},\n",
       "  {'str': 'for', 'lemma': 'for', 'POS': 'ADP'}],\n",
       " [{'str': 'comment', 'lemma': 'comment', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'planning', 'lemma': 'plan', 'POS': 'VERB'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'provided', 'lemma': 'provide', 'POS': 'VERB'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'},\n",
       "  {'str': 'should', 'lemma': 'should', 'POS': 'AUX'},\n",
       "  {'str': 'charge', 'lemma': 'charge', 'POS': 'VERB'}],\n",
       " [{'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'get', 'lemma': 'get', 'POS': 'VERB'},\n",
       "  {'str': 'up', 'lemma': 'up', 'POS': 'ADP'},\n",
       "  {'str': 'go', 'lemma': 'go', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': 'stopped', 'lemma': 'stop', 'POS': 'VERB'},\n",
       "  {'str': 'wondered', 'lemma': 'wonder', 'POS': 'VERB'},\n",
       "  {'str': 'kill', 'lemma': 'kill', 'POS': 'VERB'}],\n",
       " [{'str': 'tell', 'lemma': 'tell', 'POS': 'VERB'},\n",
       "  {'str': 'before', 'lemma': 'before', 'POS': 'ADP'},\n",
       "  {'str': 'merging', 'lemma': 'merge', 'POS': 'VERB'}],\n",
       " [{'str': 'differ', 'lemma': 'differ', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'seated', 'lemma': 'seat', 'POS': 'VERB'},\n",
       "  {'str': 'lying', 'lemma': 'lie', 'POS': 'VERB'},\n",
       "  {'str': 'down', 'lemma': 'down', 'POS': 'ADP'}],\n",
       " [{'str': 'stood', 'lemma': 'stand', 'POS': 'VERB'},\n",
       "  {'str': 'poised', 'lemma': 'poise', 'POS': 'VERB'}],\n",
       " [{'str': 'assigned', 'lemma': 'assign', 'POS': 'VERB'},\n",
       "  {'str': 'based', 'lemma': 'base', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'assigned', 'lemma': 'assign', 'POS': 'VERB'},\n",
       "  {'str': 'based', 'lemma': 'base', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'has', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'},\n",
       "  {'str': 'enthuse', 'lemma': 'enthuse', 'POS': 'VERB'}],\n",
       " [{'str': 'know', 'lemma': 'know', 'POS': 'VERB'},\n",
       "  {'str': 'Chinese', 'lemma': 'chinese', 'POS': 'ADJ'},\n",
       "  {'str': 'central', 'lemma': 'central', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'VERB'}],\n",
       " [{'str': 'know', 'lemma': 'know', 'POS': 'VERB'},\n",
       "  {'str': 'Chinese', 'lemma': 'chinese', 'POS': 'ADJ'},\n",
       "  {'str': 'central', 'lemma': 'central', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'VERB'}],\n",
       " [{'str': 'asked', 'lemma': 'ask', 'POS': 'VERB'},\n",
       "  {'str': 'about', 'lemma': 'about', 'POS': 'ADP'},\n",
       "  {'str': 'grown', 'lemma': 'grow', 'POS': 'VERB'}],\n",
       " [{'str': 'Find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'contacting', 'lemma': 'contact', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsed predicates with more then one verb per embedding predicate (more than in the golden set)\n",
    "list(pred for pred in parsed_predicates if (lambda x: len([pr for pr in pred if pr['POS'] == 'VERB'])>1)(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c293623-611f-4295-bbd0-404b9b9ee25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benepar_env",
   "language": "python",
   "name": "benepar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
