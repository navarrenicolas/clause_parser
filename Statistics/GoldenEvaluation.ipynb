{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a1a894-5b0f-47bd-af5b-d599703f5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from FullParser.ClauseParser import ClauseParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4261546-559f-4063-a1e2-cc564ad96a18",
   "metadata": {},
   "source": [
    "## Parsing tools and data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013b9739-86d3-413a-b246-5ab32e4aed54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "parser = ClauseParser()\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "# Function for quick sentence processing\n",
    "def nlp_sents(string):\n",
    "    return list(nlp(string).sents)\n",
    "\n",
    "# Golden Data file paths\n",
    "dec_path_golden = \"../Annotation/declarative_golden_set.json\"\n",
    "pol_path_golden = \".../Annotation/polar_golden_set.json\"\n",
    "alt_path_golden = \"../Annotation/alternative_golden_set.json\"\n",
    "const_path_golden = \"../Annotation/constituent_golden_set.json\"\n",
    "adv_path_golden = \"../Annotation/adversarials_golden_set.json\"\n",
    "flat_path_golden = \"../Annotation/golden_sets_flattened.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1b370-237f-47a8-9992-5d766a68bba9",
   "metadata": {},
   "source": [
    "## Parse Golden sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9883d040-97b4-4691-a344-07bccaa2a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def parse_flat_golden(filename:str):\n",
    "    golden_df = pd.read_json(filename, orient = 'index')\n",
    "    golden_parses = []\n",
    "    parser_parses = []\n",
    "    for sent in golden_df.sentence.value_counts().to_dict().keys() :\n",
    "        parses = [dict(row) for i,row in (golden_df[golden_df.sentence == sent]).iterrows()]\n",
    "        golden_parses.append(parses)\n",
    "        sent_doc = nlp(sent)\n",
    "        parsed_sent = list(sent_doc.sents)[0]\n",
    "        parser_parses.append(parser.parse_clauses(parsed_sent))\n",
    "    return (parser_parses, golden_parses)\n",
    "\n",
    "flat_parsed, flat_golden = parse_flat_golden(flat_path_golden)\n",
    "adv_parsed, adv_golden = parse_flat_golden(adv_path_golden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbd5ee-2cb4-40bf-a306-8344f99990d3",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b447a9-637d-4051-8eb9-1efe53d327b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicate_string(predicate):\n",
    "    if len(predicate) ==0:\n",
    "        return ''\n",
    "    pred_string = ''\n",
    "    for item in predicate:\n",
    "        pred_string += str(item['lemma']) + ' '\n",
    "    return pred_string[:-1]\n",
    "\n",
    "\n",
    "def filter_sentences_idx(filt):\n",
    "    return [idx for idx in [i for i, e in enumerate(flat_golden) if filt(e)] ]\n",
    "\n",
    "single_idx = filter_sentences_idx(lambda x: len(x) == 1)\n",
    "multiple_idx = filter_sentences_idx(lambda x: len(x) > 1)\n",
    "\n",
    "\n",
    "\n",
    "def compare_data(parsed,golden,feature):\n",
    "    if feature== 'type':\n",
    "        return [[any([(gp['clause'] == e['clause'] and  gp['type'] == e['type']) for e in parsed[i]]) for gp in gold]  for i,gold in enumerate(golden)]\n",
    "    if feature=='predicate':\n",
    "        return [[any([(get_predicate_string(gp['predicate']) == get_predicate_string(e['predicate'])) for e in parsed[i]]) for gp in gold]  for i,gold in enumerate(golden)]\n",
    "    return [[any([gp[feature] == e[feature] for e in parsed[i]]) for gp in gold]  for i,gold in enumerate(golden)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a34d7-850b-4d9d-aaa4-7d09c745d0c2",
   "metadata": {},
   "source": [
    "## Single Clause Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df76831-b9b2-4b23-b58a-afa8ab28ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection accuracy 0.9444444444444444\n",
      "Clause accuracy 0.8253968253968254\n",
      "Precicate accuracy 0.91005291005291\n",
      "Type accuracy 0.91005291005291\n"
     ]
    }
   ],
   "source": [
    "parsed_single = [flat_parsed[i] for i in single_idx]\n",
    "golden_single = [flat_golden[i] for i in single_idx]\n",
    "\n",
    "# Clause detection\n",
    "\n",
    "detected_clauses =  list(map(lambda x: len(x)>0,parsed_single))\n",
    "print('Detection accuracy', np.mean(detected_clauses))\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses = compare_data(parsed_single,golden_single,'type')\n",
    "print('Clause accuracy', np.mean(correct_clauses))\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(parsed_single,golden_single,'predicate')\n",
    "print('Precicate accuracy',np.mean(correct_predicates))\n",
    "\n",
    "failed_single_predicates = [golden_single[i][0]['sentence'] for i,e in enumerate(correct_predicates) if (not e[0] and correct_clauses[i])]\n",
    "\n",
    "correct_types = compare_data(parsed_single,golden_single,'type')\n",
    "print('Type accuracy',np.mean(correct_predicates))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea9fcf-ee49-42d0-a0d1-71eb3967d624",
   "metadata": {},
   "source": [
    "## Multiple Clause Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d873fc-71c6-4407-8d23-dedaafc63229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple-clauses detection accuracy 0.8253968253968254\n",
      "Multiple-clauses clause accuracy  0.8253968253968254\n",
      "Multiple-clauses predicate accuracy 0.91005291005291\n"
     ]
    }
   ],
   "source": [
    "parsed_multiple = [flat_parsed[i] for i in single_idx]\n",
    "golden_multiple = [flat_golden[i] for i in single_idx]\n",
    "\n",
    "# Detection\n",
    "\n",
    "detected_multiple = [len(gold) == len(parsed_multiple[i]) for i,gold in enumerate(golden_multiple)]\n",
    "print('Multiple-clauses detection accuracy', np.mean(correct_clauses))\n",
    "\n",
    "failed_multiple_detect = [parsed_multiple[i] for i,detect in enumerate(detected_multiple) if not detect]\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses = compare_data(parsed_multiple,golden_multiple,'type')\n",
    "print('Multiple-clauses clause accuracy ',np.mean(list(map(np.mean,correct_clauses))))\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(parsed_multiple,golden_multiple,'predicate')\n",
    "print('Multiple-clauses predicate accuracy',np.mean(list(map(np.mean,correct_predicates))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c22b13-b281-4c3e-bd30-01daa1a4958d",
   "metadata": {},
   "source": [
    "## Overall Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ff59ad-746d-4f1e-ad67-dcaea65675ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentence accuracy 0.9351230425055929\n",
      "Overall detection accuracy 0.9440715883668904\n",
      "Overall clause accuracy 0.785234899328859\n",
      "Overall predicate accuracy 0.8859060402684564\n"
     ]
    }
   ],
   "source": [
    "# Failed sentence reproduction\n",
    "same_sentences = compare_data(flat_parsed,flat_golden,'sentence')\n",
    "print('Overall sentence accuracy', np.mean(list(map(all,same_sentences))))\n",
    "\n",
    "# Clause detection\n",
    "\n",
    "detected_clauses = list(map(lambda x: len(x)>0,flat_parsed))\n",
    "print('Overall detection accuracy', np.mean(detected_clauses))\n",
    "\n",
    "failed_detects = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if not detected_clauses[i]]\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses = compare_data(flat_parsed,flat_golden,'type')\n",
    "print('Overall clause accuracy', np.mean(list(map(all,correct_clauses))))\n",
    "\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates = compare_data(flat_parsed,flat_golden,'predicate')\n",
    "print('Overall predicate accuracy', np.mean(list(map(all,correct_predicates))))\n",
    "\n",
    "failed_clauses = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if (detected_clauses[i] and  not any(correct_clauses[i]) and all(correct_predicates[i]))] \n",
    "\n",
    "\n",
    "failed_preds = [gp[0]['sentence'] for i,gp in enumerate(flat_golden) if (all(same_sentences[i]) and detected_clauses[i] and  not any(correct_predicates[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be361408-f219-493b-9b45-dfe86c5964a1",
   "metadata": {},
   "source": [
    "## Single predicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628bcc2f-7ca9-457f-ba6b-075496311996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection accuracy 0.8233695652173914\n",
      "clause accuracy  0.8075181159420289\n",
      "predicate accuracy 0.9211956521739131\n"
     ]
    }
   ],
   "source": [
    "single_preds_idx = filter_sentences_idx(lambda x: any([len(c['predicate']) == 1 for c in x]))\n",
    "\n",
    "parsed_single_pred = [flat_parsed[i] for i in single_preds_idx]\n",
    "golden_single_pred = [flat_golden[i] for i in single_preds_idx]\n",
    "\n",
    "# Detection\n",
    "\n",
    "detected_single_pred = [len(gold) == len(parsed_single_pred[i]) for i,gold in enumerate(golden_single_pred)]\n",
    "print('detection accuracy', np.mean(detected_single_pred))\n",
    "\n",
    "\n",
    "# clause and type\n",
    "\n",
    "correct_clauses_single_pred = compare_data(parsed_single_pred,golden_single_pred,'type')\n",
    "print('clause accuracy ',np.mean(list(map(np.mean,correct_clauses_single_pred))))\n",
    "\n",
    "# Predicate detection\n",
    "\n",
    "correct_predicates_single_pred = compare_data(parsed_single_pred,golden_single_pred,'predicate')\n",
    "print('predicate accuracy',np.mean(list(map(all,correct_predicates_single_pred))))\n",
    "\n",
    "\n",
    "failed_single_preds = [gp[0]['sentence'] for i,gp in enumerate(golden_single_pred) if (detected_single_pred[i] and  not any(correct_predicates_single_pred[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5f847-3445-4aca-bb4f-94d9bb6740e8",
   "metadata": {},
   "source": [
    "## Adversarial sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1d884f-051e-4d3c-9fc7-c4049524fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = [parse for parse in adv_parsed if len(parse)>0]\n",
    "false_positive_sentences = [parse[0]['sentence'] for parse in false_positives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28a4cc5-5cfa-4e19-b6bd-39c0a8244ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The phone call gets passed around a number of confused staff members, which adds to the hilarity of the video',\n",
       " 'And if you do not want to purchase clothing, I do have a few clothing options if you’d like to come to the studio to take a look.',\n",
       " 'March 27, 2019, Christian County, US 6: A 67 year-old Reeds Spring, MO man was killed when a Chevy Impala crossed the median and struck his Harley Davidson motorcycle, two other motorcyclist were seriously injured as one was also hit head-on and another motorcyclist was struck by debris',\n",
       " 'March 25, 2019; Stone County, MO 143: An 82 year-old Crane, MO woman was killed when her Chevy HHR was struck head-on by a Ford Ranger driven by a 68 year-old Marionville, MO man',\n",
       " 'February 17, 2019, Adair County, Hwy 6: A 34 year-old Brashear, MO man was seriously injured along with his 10 year-old son when a 28 year-old Canton, MO man crossed the center line on icy highway and struck their Jeep head-on',\n",
       " 'Louis, MO woman and an 18 year-old Hannibal, MO woman were both seriously injured when a Toyota Matrix driven by an 18 year-old Country Club Hills, IL man crossed the center line and struck their Mazda.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505887b0-83c5-4669-9624-2edc15550685",
   "metadata": {},
   "source": [
    "## Failure Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a2a99-91c4-47af-8573-60f2b68f1518",
   "metadata": {},
   "source": [
    "### Some useful fonctions for probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c0f693-0696-4907-9c84-0a211ed9ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_brackets(parsed_string):\n",
    "    # Replace round brackets with square brackets\n",
    "    replaced_string = parsed_string.replace('(', '[').replace(')', ']')\n",
    "    # Wrap the string with \\begin{forest} and \\end{forest}\n",
    "    final_string = '\\\\begin{adjustbox}{width=0.8\\\\linewidth}' + '\\\\begin{forest} ' + replaced_string + ' \\\\end{forest}' + '\\\\end{adjustbox}\\\\\\\\'\n",
    "    return final_string\n",
    "\n",
    "def copy_latex_parse(sentence):\n",
    "    ps = list(nlp(sentence).sents)[0]\n",
    "    return replace_brackets(ps._.parse_string)\n",
    "\n",
    "# Find parse of sentences matching keywords\n",
    "def find_parse(string):\n",
    "    return [parse for parse in flat_parsed if (lambda x: (string in x[0]['sentence']) if len(x) > 0 else False)(parse)]\n",
    "# Find golden parse of sentences matching keywords\n",
    "def find_golden_parse(string):\n",
    "    return [gp for gp in flat_golden if (string in gp[0]['sentence'])]\n",
    "\n",
    "import nltk\n",
    "def nlp_parse(sent):\n",
    "    return nltk.Tree.fromstring(list(nlp(sent).sents)[0]._.parse_string).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "123a4308-c3b7-4ee2-94e1-d01a6d2f8787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                          S                                                                         \n",
      "              ____________________________________________________________________________|_______________________________________________________________________   \n",
      "             |                                  |         S                                                                                                       | \n",
      "             |                                  |     ____|_________________                                                                                      |  \n",
      "             |                                  |    |                      VP                                                                                    | \n",
      "             |                                  |    |     _________________|_____________                                                                        |  \n",
      "             |                                  |    |    |                               VP                                                                      | \n",
      "             |                                  |    |    |         ______________________|_____________________                                                  |  \n",
      "             |                                  |    |    |        |                                            PP                                                | \n",
      "             |                                  |    |    |        |                   _________________________|_____                                            |  \n",
      "             |                                  |    |    |        |                  |               |   |          SBAR                                         | \n",
      "             |                                  |    |    |        |                  |               |   |      _____|____                                       |  \n",
      "             S                                  |    |    |        |                  |               |   |     |          S                                      | \n",
      "   __________|__________                        |    |    |        |                  |               |   |     |      ____|____                                  |  \n",
      "  |                     VP                      |    |    |        |                  |               |   |     |     |         VP                                | \n",
      "  |     ________________|____                   |    |    |        |                  |               |   |     |     |     ____|______                           |  \n",
      "  |    |                     NP                 |    |    |        |                  |               |   |     |     |    |           VP                         | \n",
      "  |    |            _________|_______           |    |    |        |                  |               |   |     |     |    |     ______|_____                     |  \n",
      "  |    |           |                 PP         |    |    |        |                  PP              |   |     |     |    |    |            VP                   | \n",
      "  |    |           |              ___|____      |    |    |        |         _________|___            |   |     |     |    |    |       _____|________            |  \n",
      "  NP   |           NP            |        NP    |    NP   |        |        |             NP          |   |     |     NP   |    |      VP    |        VP          | \n",
      "  |    |      _____|____         |        |     |    |    |        |        |     ________|_____      |   |     |     |    |    |      |     |    ____|_____      |  \n",
      "  EX  VBP    JJ        NNS       IN      NNS    CC   DT  VBP      VBN       IN  PRP$      NN    NN    CC  CC    IN   PRP  VBP  VBN    VBN    CC  NN        VBN    . \n",
      "  |    |     |          |        |        |     |    |    |        |        |    |        |     |     |   |     |     |    |    |      |     |   |          |     |  \n",
      "There are various     types      of     creams and these are distinguished  by their     fat content and  or whether they have been whipped  or heat     treated  . \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/benepar_env/lib/python3.12/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nlp_parse(find_parse('are distinguished')[0][0]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b725f09a-ea70-40f2-af09-bfc03ce1adbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At the post-secondary level, students fall into four categories, depending on whether their work placement is paid or unpaid, and whether their work placement is optional or a mandatory requirement for graduation.',\n",
       " \"Doctors and medical malpractice insurance companies currently must live with this 'bad outcome' ruling, although it is often unknown whether a better outcome was possible.\",\n",
       " \"I still get confused as to whether I'm being selfish and in typing this comment I can actually feel myself looking for affirmation from you and other posters.\",\n",
       " 'Multiplayer will include objectives that will differ depending on whether a player has chosen to fight for the Allies or Axis.',\n",
       " 'That said, it matters little whether I am over-eating or under-eating.',\n",
       " 'It is unclear to us whether the word \"new\" in this measure applies only to MOUs or whether it also applies to other pension contracts or agreements, such as those delineated in statute or those applicable to managers and supervisors.',\n",
       " 'It seems to be random whether these questions are relatively easy or impossibly difficult.',\n",
       " 'The other two cats stood poised nearby, as if undecided on whether to back up their fellow or choose the better part of valor.',\n",
       " 'There are various types of creams and these are distinguished by their fat content and or whether they have been whipped or heat treated.',\n",
       " 'The distance will be positive or negative depending on whether the point is outside or inside the circle, but this does not matter since the value is squared as part of the minimization process.',\n",
       " 'The Board members who were aware of the article should have inquired further about Sandusky and the possible risks of litigation or public relations issues, and, most importantly, whether the University has effective policies in place to protect children on its campuses.',\n",
       " 'Al Hamili remained non-committal on whether or not Opec would increase output in view of the of the current market volatility.',\n",
       " \"We're not sure whether pollsters were referring to Los Stop's version, Con su blanca palidez, though somehow we suspect not.\",\n",
       " 'Different weights are assigned to the words based on whether they appear in the title, subheadings or the body of the page and the number of times a particular word appears in the webpage is stored.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1951062a-8280-4784-b9a5-b1392df3384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence It seems to be random whether these questions are relatively easy or impossibly difficult. \n",
      "\n",
      "Golden [([{'str': 'be', 'lemma': 'be', 'POS': 'AUX'}, {'str': 'random', 'lemma': 'random', 'POS': 'ADJ'}], 'whether these questions are relatively easy or impossibly difficult')]\n",
      "\n",
      "\n",
      "Parsed [([{'str': 'seems', 'lemma': 'seem', 'POS': 'VERB'}, {'str': 'be', 'lemma': 'be', 'POS': 'AUX'}, {'str': 'random', 'lemma': 'random', 'POS': 'ADJ'}], 'whether these questions are relatively easy or impossibly difficult')]\n",
      "                                                   S                                                                \n",
      "  _________________________________________________|______________________________________________________________   \n",
      " |             VP                                                                                                 | \n",
      " |     ________|___________________________________                                                               |  \n",
      " |    |        |                                  SBAR                                                            | \n",
      " |    |        |                 __________________|_____________                                                 |  \n",
      " |    |        S                |                                S                                                | \n",
      " |    |        |                |            ____________________|____________                                    |  \n",
      " |    |        VP               |           |                                 VP                                  | \n",
      " |    |     ___|___             |           |              ___________________|____                               |  \n",
      " NP   |    |       VP           |           |             |                       ADJP                            | \n",
      " |    |    |    ___|____        |           |             |               _________|_______________               |  \n",
      " NP   |    |   |       ADJP     |           NP            |             ADJP       |              ADJP            | \n",
      " |    |    |   |        |       |       ____|______       |       _______|____     |        _______|_______       |  \n",
      "PRP  VBZ   TO  VB       JJ      IN     DT         NNS    VBP     RB           JJ   CC      RB              JJ     . \n",
      " |    |    |   |        |       |      |           |      |      |            |    |       |               |      |  \n",
      " It seems  to  be     random whether these     questions are relatively      easy  or  impossibly      difficult  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the golden parses matching query\n",
    "fail_idx = \n",
    "fail_sentences = failed_preds\n",
    "print('sentence', fail_sentences[fail_idx],'\\n')\n",
    "\n",
    "print('Golden',[(gp['predicate'], gp['clause']) for gp in find_golden_parse(fail_sentences[fail_idx])[0]])\n",
    "print('\\n')\n",
    "print('Parsed',[(p['predicate'],p['clause']) for p in find_parse(fail_sentences[fail_idx])[0]])\n",
    "\n",
    "# parser.parse_clauses(nlp_sents(failed_preds[fail_idx])[0])\n",
    "nlp_parse(fail_sentences[fail_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63382c68-aea9-42b3-9341-5a1881ef3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pyperclip \n",
    "\n",
    "trees = ''\n",
    "for sentence in failed_preds:\n",
    "    trees += replace_brackets(list(nlp(sentence).sents)[0]._.parse_string) + '\\n' \n",
    "pyperclip.copy(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77844ba1-5db0-45f7-8894-65dfc9a54e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "know         76\n",
       "decide       34\n",
       "say          32\n",
       "ask          22\n",
       "tell         17\n",
       "think        17\n",
       "wonder       17\n",
       "see          15\n",
       "determine    14\n",
       "discuss      12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_clauses = []\n",
    "for gold in flat_golden:\n",
    "    for gp in gold:\n",
    "        gold_clauses.append(gp['clause'])\n",
    "\n",
    "# Quick reflection of the golden single predicates\n",
    "gold_predicates = []\n",
    "for gold in flat_golden:\n",
    "    for gp in gold:\n",
    "        gold_predicates.append(gp['predicate'])\n",
    "gold_single_preds = pd.Series([pred[0]['lemma'] for pred in  filter(lambda x: len(x)==1,gold_predicates)])\n",
    "gold_single_preds.value_counts()[0:10]\n",
    "\n",
    "\n",
    "[pred for pred in gold_predicates if  len(pred)>2]\n",
    "\n",
    "# Quick reflection of the parsed single predicates\n",
    "parsed_predicates = []\n",
    "for sent_parse in flat_parsed:\n",
    "    for parse in sent_parse:\n",
    "        parsed_predicates.append(parse['predicate'])\n",
    "parsed_single_preds = pd.Series([pred[0]['lemma'] for pred in  filter(lambda x: len(x)==1,parsed_predicates)])\n",
    "parsed_single_preds.value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2e91389-f634-4a36-a074-a8cba6a4ea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'str': 'says', 'lemma': 'say', 'POS': 'VERB'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'AUX'},\n",
       "  {'str': 'enjoyed', 'lemma': 'enjoy', 'POS': 'VERB'},\n",
       "  {'str': 'working', 'lemma': 'work', 'POS': 'VERB'},\n",
       "  {'str': 'for', 'lemma': 'for', 'POS': 'ADP'}],\n",
       " [{'str': \"'s\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'impossible', 'lemma': 'impossible', 'POS': 'ADJ'},\n",
       "  {'str': 'find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'half', 'lemma': 'half', 'POS': 'ADJ'},\n",
       "  {'str': 'is', 'lemma': 'be', 'POS': 'AUX'}],\n",
       " [{'str': 'provided', 'lemma': 'provide', 'POS': 'VERB'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'},\n",
       "  {'str': 'should', 'lemma': 'should', 'POS': 'AUX'},\n",
       "  {'str': 'charge', 'lemma': 'charge', 'POS': 'VERB'}],\n",
       " [{'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'get', 'lemma': 'get', 'POS': 'VERB'},\n",
       "  {'str': 'up', 'lemma': 'up', 'POS': 'ADP'},\n",
       "  {'str': 'go', 'lemma': 'go', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': 'be', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'positive', 'lemma': 'positive', 'POS': 'ADJ'},\n",
       "  {'str': 'negative', 'lemma': 'negative', 'POS': 'ADJ'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'inquired', 'lemma': 'inquire', 'POS': 'VERB'},\n",
       "  {'str': 'about', 'lemma': 'about', 'POS': 'ADP'},\n",
       "  {'str': 'possible', 'lemma': 'possible', 'POS': 'ADJ'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'}],\n",
       " [{'str': 'remained', 'lemma': 'remain', 'POS': 'VERB'},\n",
       "  {'str': 'non', 'lemma': 'non', 'POS': 'ADJ'},\n",
       "  {'str': '-', 'lemma': '-', 'POS': 'ADJ'},\n",
       "  {'str': 'committal', 'lemma': 'committal', 'POS': 'ADJ'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'sure', 'lemma': 'sure', 'POS': 'ADJ'},\n",
       "  {'str': 'were', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'referring', 'lemma': 'refer', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'sure', 'lemma': 'sure', 'POS': 'ADJ'},\n",
       "  {'str': 'were', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'referring', 'lemma': 'refer', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': \"'m\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'interested', 'lemma': 'interested', 'POS': 'ADJ'},\n",
       "  {'str': 'in', 'lemma': 'in', 'POS': 'ADP'},\n",
       "  {'str': 'spammy', 'lemma': 'spammy', 'POS': 'ADJ'},\n",
       "  {'str': 'commercial', 'lemma': 'commercial', 'POS': 'ADJ'},\n",
       "  {'str': \"'re\", 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'proposing', 'lemma': 'propose', 'POS': 'VERB'}],\n",
       " [{'str': 'Find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'contacting', 'lemma': 'contact', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsed predicates with more than 4 items (many more than the golden set)\n",
    "list(pred for pred in parsed_predicates if len(pred)>4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "753c0c86-aa84-4c25-a033-8df59da32d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'str': 'says', 'lemma': 'say', 'POS': 'VERB'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'AUX'},\n",
       "  {'str': 'enjoyed', 'lemma': 'enjoy', 'POS': 'VERB'},\n",
       "  {'str': 'working', 'lemma': 'work', 'POS': 'VERB'},\n",
       "  {'str': 'for', 'lemma': 'for', 'POS': 'ADP'}],\n",
       " [{'str': 'comment', 'lemma': 'comment', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'planning', 'lemma': 'plan', 'POS': 'VERB'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'fall', 'lemma': 'fall', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'provided', 'lemma': 'provide', 'POS': 'VERB'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'much', 'lemma': 'much', 'POS': 'ADJ'},\n",
       "  {'str': 'public', 'lemma': 'public', 'POS': 'ADJ'},\n",
       "  {'str': 'should', 'lemma': 'should', 'POS': 'AUX'},\n",
       "  {'str': 'charge', 'lemma': 'charge', 'POS': 'VERB'}],\n",
       " [{'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'care', 'lemma': 'care', 'POS': 'VERB'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'get', 'lemma': 'get', 'POS': 'VERB'},\n",
       "  {'str': 'up', 'lemma': 'up', 'POS': 'ADP'},\n",
       "  {'str': 'go', 'lemma': 'go', 'POS': 'VERB'},\n",
       "  {'str': 'to', 'lemma': 'to', 'POS': 'ADP'}],\n",
       " [{'str': 'stopped', 'lemma': 'stop', 'POS': 'VERB'},\n",
       "  {'str': 'wondered', 'lemma': 'wonder', 'POS': 'VERB'},\n",
       "  {'str': 'kill', 'lemma': 'kill', 'POS': 'VERB'}],\n",
       " [{'str': 'tell', 'lemma': 'tell', 'POS': 'VERB'},\n",
       "  {'str': 'before', 'lemma': 'before', 'POS': 'ADP'},\n",
       "  {'str': 'merging', 'lemma': 'merge', 'POS': 'VERB'}],\n",
       " [{'str': 'differ', 'lemma': 'differ', 'POS': 'VERB'},\n",
       "  {'str': 'depending', 'lemma': 'depend', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'seated', 'lemma': 'seat', 'POS': 'VERB'},\n",
       "  {'str': 'lying', 'lemma': 'lie', 'POS': 'VERB'},\n",
       "  {'str': 'down', 'lemma': 'down', 'POS': 'ADP'}],\n",
       " [{'str': 'stood', 'lemma': 'stand', 'POS': 'VERB'},\n",
       "  {'str': 'poised', 'lemma': 'poise', 'POS': 'VERB'}],\n",
       " [{'str': 'assigned', 'lemma': 'assign', 'POS': 'VERB'},\n",
       "  {'str': 'based', 'lemma': 'base', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'assigned', 'lemma': 'assign', 'POS': 'VERB'},\n",
       "  {'str': 'based', 'lemma': 'base', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'}],\n",
       " [{'str': 'has', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'},\n",
       "  {'str': 'enthuse', 'lemma': 'enthuse', 'POS': 'VERB'}],\n",
       " [{'str': 'know', 'lemma': 'know', 'POS': 'VERB'},\n",
       "  {'str': 'Chinese', 'lemma': 'chinese', 'POS': 'ADJ'},\n",
       "  {'str': 'central', 'lemma': 'central', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'VERB'}],\n",
       " [{'str': 'know', 'lemma': 'know', 'POS': 'VERB'},\n",
       "  {'str': 'Chinese', 'lemma': 'chinese', 'POS': 'ADJ'},\n",
       "  {'str': 'central', 'lemma': 'central', 'POS': 'ADJ'},\n",
       "  {'str': 'has', 'lemma': 'have', 'POS': 'VERB'}],\n",
       " [{'str': 'asked', 'lemma': 'ask', 'POS': 'VERB'},\n",
       "  {'str': 'about', 'lemma': 'about', 'POS': 'ADP'},\n",
       "  {'str': 'grown', 'lemma': 'grow', 'POS': 'VERB'}],\n",
       " [{'str': 'Find', 'lemma': 'find', 'POS': 'VERB'},\n",
       "  {'str': 'out', 'lemma': 'out', 'POS': 'ADP'},\n",
       "  {'str': 'by', 'lemma': 'by', 'POS': 'ADP'},\n",
       "  {'str': 'contacting', 'lemma': 'contact', 'POS': 'VERB'},\n",
       "  {'str': 'of', 'lemma': 'of', 'POS': 'ADP'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}],\n",
       " [{'str': 'reflect', 'lemma': 'reflect', 'POS': 'VERB'},\n",
       "  {'str': 'on', 'lemma': 'on', 'POS': 'ADP'},\n",
       "  {'str': 'lucky', 'lemma': 'lucky', 'POS': 'ADJ'},\n",
       "  {'str': 'am', 'lemma': 'be', 'POS': 'AUX'},\n",
       "  {'str': 'have', 'lemma': 'have', 'POS': 'VERB'},\n",
       "  {'str': 'such', 'lemma': 'such', 'POS': 'ADJ'},\n",
       "  {'str': 'close', 'lemma': 'close', 'POS': 'ADJ'},\n",
       "  {'str': 'with', 'lemma': 'with', 'POS': 'ADP'},\n",
       "  {'str': 'extended', 'lemma': 'extend', 'POS': 'VERB'}]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsed predicates with more then one verb per embedding predicate (more than in the golden set)\n",
    "list(pred for pred in parsed_predicates if (lambda x: len([pr for pr in pred if pr['POS'] == 'VERB'])>1)(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c293623-611f-4295-bbd0-404b9b9ee25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benepar_env",
   "language": "python",
   "name": "benepar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
